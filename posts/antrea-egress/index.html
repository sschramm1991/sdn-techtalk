

<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="UTF-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="robots" content="index, follow"><link rel="author" href="/humans.txt">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<meta name="msapplication-TileImage" content="/mstile-144x144.png">
<meta name="theme-color" content="#494f5c">
<meta name="msapplication-TileColor" content="#494f5c">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#494f5c"><meta name="author" content="Steven Schramm">

  <meta itemprop="name" content="Controlling the egress IPs for K8s Pods within Tanzu">
  <meta itemprop="description" content="Introduction The integration of K8s platforms within a enterprise network environment is always a challenge regarding security and routing of certain K8s pods. Multiple pods are running in the same K8s cluster but might have different functinalities and requirements to access services outside of the K8s platform. What should you do, if you are unable to assign a specific outgoing IP to a specific pod or group of pods? In this case you are forced to treat all the pods within a specific K8s cluster the same way and create firewall rules based on this. That does mean all the different pods within a K8s cluster will have network permissions and it is very challenging to implement further isolation.">
  <meta itemprop="datePublished" content="2024-12-21T19:54:18+01:00">
  <meta itemprop="dateModified" content="2024-12-21T19:54:18+01:00">
  <meta itemprop="wordCount" content="1919">
  <meta itemprop="keywords" content="Homelab,Nested Lab,Vmware,Esxi,Network,Tanzu,Antrea,Nsx"><meta property="og:url" content="https://sdn-techtalk.com/posts/antrea-egress/">
  <meta property="og:site_name" content="SDN-Techtalk | Steven Schramm">
  <meta property="og:title" content="Controlling the egress IPs for K8s Pods within Tanzu">
  <meta property="og:description" content="Introduction The integration of K8s platforms within a enterprise network environment is always a challenge regarding security and routing of certain K8s pods. Multiple pods are running in the same K8s cluster but might have different functinalities and requirements to access services outside of the K8s platform. What should you do, if you are unable to assign a specific outgoing IP to a specific pod or group of pods? In this case you are forced to treat all the pods within a specific K8s cluster the same way and create firewall rules based on this. That does mean all the different pods within a K8s cluster will have network permissions and it is very challenging to implement further isolation.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-21T19:54:18+01:00">
    <meta property="article:modified_time" content="2024-12-21T19:54:18+01:00">
    <meta property="article:tag" content="Homelab">
    <meta property="article:tag" content="Nested Lab">
    <meta property="article:tag" content="Vmware">
    <meta property="article:tag" content="Esxi">
    <meta property="article:tag" content="Network">
    <meta property="article:tag" content="Tanzu">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Controlling the egress IPs for K8s Pods within Tanzu">
  <meta name="twitter:description" content="Introduction The integration of K8s platforms within a enterprise network environment is always a challenge regarding security and routing of certain K8s pods. Multiple pods are running in the same K8s cluster but might have different functinalities and requirements to access services outside of the K8s platform. What should you do, if you are unable to assign a specific outgoing IP to a specific pod or group of pods? In this case you are forced to treat all the pods within a specific K8s cluster the same way and create firewall rules based on this. That does mean all the different pods within a K8s cluster will have network permissions and it is very challenging to implement further isolation.">

<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Controlling the egress IPs for K8s Pods within Tanzu",
    "name": "Controlling the egress IPs for K8s Pods within Tanzu",
    "description": "Introduction The integration of K8s platforms within a enterprise network environment is always a challenge regarding security and routing of certain K8s pods. Multiple pods are running in the same K8s cluster but might have different functinalities and requirements to access services outside of the K8s platform. What should you do, if you are unable to assign a specific outgoing IP to a specific pod or group of pods? In this case you are forced to treat all the pods within a specific K8s cluster the same way and create firewall rules based on this. That does mean all the different pods within a K8s cluster will have network permissions and it is very challenging to implement further isolation.\n",
    "keywords": ["homelab", "nested lab", "vmware", "esxi", "network", "tanzu", "antrea", "nsx"],
    "articleBody": "Introduction The integration of K8s platforms within a enterprise network environment is always a challenge regarding security and routing of certain K8s pods. Multiple pods are running in the same K8s cluster but might have different functinalities and requirements to access services outside of the K8s platform. What should you do, if you are unable to assign a specific outgoing IP to a specific pod or group of pods? In this case you are forced to treat all the pods within a specific K8s cluster the same way and create firewall rules based on this. That does mean all the different pods within a K8s cluster will have network permissions and it is very challenging to implement further isolation.\nFor legacy applications which were deployed on physical servers or specific virtual servers, each of those servers got a specific IP assigned. Based on this it was possible to create specific firewall rules for teh specific servers based on the different requirements to access other services or beeing accessed from other systems. This brought some isolation and overall security to the enterprise networks and is also a demand to satisfy the guidelines of some specific authorities like BAFIN or based on DORA.\nWhy is this now a challenge within K8s? K8s is an additinal layer of abstraction to increase the efficiency to deploy different services on a single server. The layer of K8s will be installed ontop of an operationg system, which is either installed on a physical or virtual server. Since the most enterprised does have a high ratio of virtualisation, it is more likli that K8s is deployed ontop of VM or bunch of VMs to build a K8s cluster. Each of those VMs is now called K8s worker or master and will run one or pultiple K8s services. In this case all the different VMs still have a specific IP adress that can be used in firewall rules, but the K8s services are getting deployed ontop of thoese VMs. That means multiple K8s services are sharing the same group of VMs and if a K8s service wants to reach a different service outside of K8s it will leave the K8s worker or master by using the IP of those VMs.\nBut in K8s there are multiple solutions for this challenge. I would like to tell you about one solution that can be used in the K8s platform by VMware Tanzu (VKS) and requires the K8s network plugin (CNI) “Antrea”. The specific feature is called “Antrea Engress”.\nI will not show how to deploy the Tanzu supervisor cluster, but how to create a K8s guest cluster with the required features enabled and how to assign specific egress IPs to specific K8s pods.\nLab environment For the tests I worked with the following lab environment.\nThree nested ESXi hosts of version 8.0.3, 24022510 vCenter server of version 8.0.3, 24022515 NSX Datacenter version 4.2.1.1 Tanzu supervisor cluster integrated with NSX Datacenter Requirements My tests are based on assigning seperate egress subnets to different K8s pods within the same K8s cluster. To implement this solution the following versions are required.\nTKr 1.31 TKG Services 3.2.0 Further it is required to deploy Tanzu with NSX as networking layer and using Antrea as CNI. For NSX a specific feature called “Child Segments” will be used to get this implementation done, so take care you current NSX version is supporting this feature.\nThe next requirement is optional, but influence the overall configuration that needs to be done manually. If you are using NAT mode, you need to manually create NoSNAT rules for the NSX child segments that will be cerated within the process. If NAT mode is disabled, this manuall configuration is not required.\nNSX child segments Before digging into the steps how to assign seperate egress subnets to different K8s pods, I will shortly describe whar NSX child segments are. If you are not aware of NSX and the routers within NSX, you need to familiarize with that first. Therefore I would recommend the NSX reference design guide https://community.broadcom.com/viewdocument/nsx-reference-design-guide-42-v10?CommunityKey=b76535ef-c5a2-474d-8270-3e83685f020e\u0026tab=librarydocuments.\nNSX child segents are based on a parent segment which is connected to a T1 router. The child segment it self can be either connected to the same or a different T1 router than the parent segment. The connection between the parent and the child segment will be done by a VLAN tag and two child segments cannot share the same VLAN tag.\nThis feature is currently not available in the UI and those child segments can be wither created through the NSX API or the Antrea NSX control application.\nBased on this feature different pods can be assigned to different subnets without the requirement of assigning different portgroups to the K8s worker and master nodes. Those subnets also does not interfere withe the reserved IPs for the assigned namespace CIDR of the K8s clusters.\nMore information of the child sgements can be discoverd in the documentation of Broadcom. https://techdocs.broadcom.com/us/en/vmware-cis/nsx/vmware-nsx/4-2/administration-guide/segments/creating-a-child-segment.html\nPrepering the K8s guest cluster Before creating the K8s cluster intself, it is required to create the Antrea configuration to enable the feature “EgressSeperateSubnet”. The following example shows an example configuration. This configuration does not negate the default settings of Antrea, which are not mentioned in this YAML-File. I also added the NSX integration in my configuration to integrate the NSX Distributed Firewall with Antrea, but this will not be further discussed in this article. Further the feature of “NodePortLocal” is also not required for the assignment of dedicated egress subnets.\nThe name of the K8s object “AntreaConfig” is very important and should match the prefix -antrea-package, otherwhise the Antrea config will not be assigned. Further this configuration needs to be added before the K8s cluster is created. It is also possible to adjust the configuration after the deployment of the K8s cluster, but this is much more complex and required a reboot of the K8s nodes.\napiVersion: cni.tanzu.vmware.com/v1alpha1\rkind: AntreaConfig\rmetadata:\rname: tkc-test01-antrea-package #prefix required\rnamespace: test\rspec:\rantrea:\rconfig:\rfeatureGates:\rEgressSeparateSubnet: true\rNodePortLocal: true\rantreaNSX:\renable: true #false by default After the configuration is applied, you can start with the K8s cluster deployment. Following my example K8s cluster YAML-File.\napiVersion: run.tanzu.vmware.com/v1alpha3\rkind: TanzuKubernetesCluster\rmetadata:\rname: tkc-test01\rnamespace: test\rspec:\rtopology:\rcontrolPlane:\rreplicas: 1\rvmClass: best-effort-medium\rstorageClass: labnfs\rtkr:\rreference:\r#name: v1.29.4---vmware.3-fips.1-tkg.1\rname: v1.31.1---vmware.2-fips-vkr.2\rnodePools:\r- replicas: 2\rname: worker\rvmClass: best-effort-medium\rstorageClass: labnfs\rsettings:\rnetwork:\rpods:\rcidrBlocks: [\"192.168.0.0/16\"]\rservices:\rcidrBlocks: [\"10.196.0.0/12\"] Creating test K8s deployments As a next step I created two namespaces to assign two different egress subnets later on.\nNamespace 1: prod Namespace 2: staging In each of those namespaces I created a test deployment based on the follwoing YAML-File.\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: web\rspec:\rreplicas: 2\rselector:\rmatchLabels:\rapp: web\rtemplate:\rmetadata:\rlabels:\rapp: web\rspec:\rcontainers:\r- name: nginx\rimage: nginx:latest The commands used for this deployment are shown below.\nCreating namespaces:\nkubectl create ns Creating deployment:\nkubectl apply -f web.yaml -n In addition to the K8s deployments I deployed a docker container outside my NSX environment based on the following steps.\nCreate folder your Dockerfile and create the Dockerfile FROM tiangolo/uvicorn-gunicorn-fastapi:python3.9\rCOPY ./app /app Create a subolder called “app” Create a file with the name “main.py” in the subfolder “app” with the content below. from fastapi import FastAPI, Request\rfrom fastapi.responses import HTMLResponse\rapp = FastAPI()\r@app.get(\"/\", response_class=HTMLResponse)\rasync def index(request: Request):\rclient_host = request.client.host\rreturn \"Requester IP: \" + client_host + \"\\n\" Go back to the folder where the Dockerfile is located and build the docker image docker build -t showfastip . Run the docker imaged with a choosen port, which is not in use. I decided to use external port 88 docker run -d -p 88:80 showfastip Create child segments I decided to create the child segments with the commandline tool “antreansxctl”. This commandline tool can be either downloaded to your local PC or you can connect to the Antrea pod of your K8s cluster to execute the desired command.\nIt's important to note that the antreansxctl commandline tool is provided in the internetworking pod\rThis Pod will be deployed, if you enable the feature antreaNSX show above in the AntreaConfig example.\rA connection to the internetworking pod can be done by the kubectl command below.\nkubectl exec -n vmware-system-antrea interworking-5bdd8f5b5f-wd6wg -c mp-adapter -it -- bash The command to create a child segment requires the follwoing information.\nPath of the parent segment: can be gathered via NSX API by the code “curl –location –request GET ‘https://nsx.lab.home/policy/api/v1/infra/segments/' –header ‘Content-Type: application/json’ –header ‘Authorization: Basic ’” NSX Manager IP NSX Manager User and Password VLAN ID used to tag the child segment (this VLAN ID will not be visible outside of NSX and will to overlap with any VLAN on the underlay network) CIDR and Gateway IP of the child segmnet (the gateway IP will be created on the T1 router used for the child segment, which is the same T1 as for the parent T1 in this case) Command description:\nantreansxctl child-segment-create --nsx-managers= --user=admin --password='' --cidr=\"\" --gateway=\"\" --parent=\"\" --vlan= An example of this command is shown below.\nantreansxctl child-segment-create --nsx-managers=10.0.1.12 --user=admin --password='VMware1!VMware1!' --cidr=\"10.100.5.0/25\" --gateway=\"10.100.5.1\" --parent=\"/infra/segments/vnet_60f55536-121d-40d0-ac48-4cb976decf3d_0\" --vlan=110 egress-prod I executed the same command for staging with some other parameters.\nantreansxctl child-segment-create --nsx-managers=10.0.1.12 --user=admin --password='VMware1!VMware1!' --cidr=\"10.100.5.128/30\" --gateway=\"10.100.5.129\" --parent=\"/infra/segments/vnet_60f55536-121d-40d0-ac48-4cb976decf3d_0\" --vlan=120 egress-staging The two child segments are now visible in the NSX UI as shown in the following picture. Create and assign antrea external ip pools I created one external IP pool for staging and one for prod.\nProd external IP Pool YAML-File:\napiVersion: crd.antrea.io/v1beta1\rkind: ExternalIPPool\rmetadata:\rname: prod-external-ip-pool\rspec:\ripRanges:\r- start: 10.100.5.10\rend: 10.100.5.20\rsubnetInfo:\rgateway: 10.100.5.1\rprefixLength: 25\rvlan: 110\rnodeSelector: {} Staging external IP pool YAML-File:\napiVersion: crd.antrea.io/v1beta1\rkind: ExternalIPPool\rmetadata:\rname: staging-external-ip-pool\rspec:\ripRanges:\r- start: 10.100.5.130\rend: 10.100.5.130\rsubnetInfo:\rgateway: 10.100.5.129\rprefixLength: 30\rvlan: 120\rnodeSelector: {} Creating the two IP pools:\nkubectl apply -f antrea-externalippool-prod.yaml\rkubectl apply -f antrea-externalippool-staging.yaml YAML-Files for assignment of prod IP pool to prod web deployment:\napiVersion: crd.antrea.io/v1beta1\rkind: Egress\rmetadata:\rname: egress-prod-web\rspec:\rappliedTo:\rnamespaceSelector:\rmatchLabels:\rkubernetes.io/metadata.name: prod\rpodSelector:\rmatchLabels:\rapp: web\rexternalIPPool: prod-external-ip-pool YAML-Files for assignment of staging IP pool to prod web deployment:\napiVersion: crd.antrea.io/v1beta1\rkind: Egress\rmetadata:\rname: egress-staging-web\rspec:\rappliedTo:\rnamespaceSelector:\rmatchLabels:\rkubernetes.io/metadata.name: staging\rpodSelector:\rmatchLabels:\rapp: web\rexternalIPPool: staging-external-ip-pool Assigning the external IP pools:\nkubectl apply -f egress-prod-web.yaml -n prod\rkubectl apply -f egress-staging-web.yaml -n staging The following command shows the assigned IPs of the two IP pools.\nkubectl get egress\rNAME EGRESSIP AGE NODE\regress-prod-web 10.100.5.10 42s tkc-test01-hsqsg-vmm2w\regress-staging-web 10.100.5.130 28s tkc-test01-worker-s9s27-qqhrg-z6jd5 Create noSNAT rule As already mentioned it is required to manually configure a noSNAT rule for the external IP pool subnets, if NAT mode is enabled for the used vSphere Namespace where the K8s cluster is deployed. The noSNAT rule needs to be configured on the T1 router where the child segments are connected as shown in the picture below.\nTesting if egress ip pool assigment is working The following tests are showing an access from the web pod in both namespaces and which external IP will be used to access the webserver deployed in dockers as mentioned earlier. The Requester IP is the egress IP visible for the target web serverrunning in docker.\nstevenschramm@Stevens-MacBook-Pro homelab % kubectl -n prod exec web-54d6b8f598-7ddqv -- curl -s 192.168.178.222:88\rRequester IP: 10.100.5.10\rstevenschramm@Stevens-MacBook-Pro homelab % kubectl -n staging exec web-54d6b8f598-slnm8 -- curl -s 192.168.178.222:88\rRequester IP: 10.100.5.130 ",
    "wordCount" : "1919",
    "inLanguage": "en",
    "datePublished": "2024-12-21T19:54:18+01:00",
    "dateModified": "2024-12-21T19:54:18+01:00",
    "author":{
        "@type": "Person",
        "name": "Steven Schramm",
        "url": "https://sdn-techtalk.com/about/"
        },
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://sdn-techtalk.com/posts/antrea-egress/"
    },
    "publisher": {
      "@type": "Organization",
      "name": "SDN-Techtalk | Steven Schramm",
      "description": "",
      "logo": {
        "@type": "ImageObject",
        "url": "https://sdn-techtalk.com/favicon.ico"
      }
    }
}
</script><title>Controlling the egress IPs for K8s Pods within Tanzu</title>
<link rel="stylesheet dns-prefetch preconnect preload prefetch" as="style" href="https://sdn-techtalk.com/css/style.min.5c8d6d17b88c7b7d66d60851d5eaab8d6264b4f1f602f57a704fac1db6716b4c.css" integrity="sha256-XI1tF7iMe31m1ghR1eqrjWJktPH2AvV6cE+sHbZxa0w=" crossorigin="anonymous">
	</head>
<body id="page">
	<header id="site-header">
		<div class="hdr-wrapper section-inner">
			<div class="hdr-left">
				<div class="site-branding">
					<a href="https://sdn-techtalk.com/">SDN-Techtalk | Steven Schramm</a>
				</div>
				<nav class="site-nav hide-in-mobile"><a href="https://sdn-techtalk.com/posts/">Posts</a><a href="https://sdn-techtalk.com/about/">About Me</a></nav>
			</div>
			<div class="hdr-right hdr-icons">
				<span class="hdr-links hide-in-mobile"><a href="https://www.linkedin.com/in/steven-schramm-87083113a" target="_blank" rel="noopener me" title="Linkedin"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none"
   stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
   <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
   <rect x="2" y="9" width="4" height="12"></rect>
   <circle cx="4" cy="4" r="2"></circle>
</svg></a></span><button id="share-btn" class="hdr-btn" title=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-share-2">
      <circle cx="18" cy="5" r="3"></circle>
      <circle cx="6" cy="12" r="3"></circle>
      <circle cx="18" cy="19" r="3"></circle>
      <line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line>
      <line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line>
   </svg></button>
 
<div id="share-links" class="animated fast">
    
    
    
    
    <ul>
        <li>
            <a href="https://twitter.com/intent/tweet?hashtags=hermit2&amp;url=https%3a%2f%2fsdn-techtalk.com%2fposts%2fantrea-egress%2f&amp;text=Controlling%20the%20egress%20IPs%20for%20K8s%20Pods%20within%20Tanzu" target="_blank" rel="noopener" aria-label="Share on X"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none"
   stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
   <path class="st0" d="m21.3 21.1 -11.4 -18.2h-7.2l11.4 18.2zm-18.6 0 7.2 -6.6m4.2 -5 7.2 -6.6" />
</svg></a>
        </li>
        <li>
            <a href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsdn-techtalk.com%2fposts%2fantrea-egress%2f" target="_blank" rel="noopener" aria-label="Share on Facebook"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none"
   stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
   <path d="M18 2h-3a5 5 0 0 0-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 0 1 1-1h3z"></path>
</svg></a>
        </li>
        <li>
            <a href="mailto:?subject=Controlling%20the%20egress%20IPs%20for%20K8s%20Pods%20within%20Tanzu&amp;body=https%3a%2f%2fsdn-techtalk.com%2fposts%2fantrea-egress%2f" target="_self" rel="noopener" aria-label="Share on Email"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none"
   stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
   <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path>
   <polyline points="22,6 12,13 2,6"></polyline>
</svg></a>
        </li>
        <li>
            <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsdn-techtalk.com%2fposts%2fantrea-egress%2f&amp;source=https%3a%2f%2fsdn-techtalk.com%2f&amp;title=Controlling%20the%20egress%20IPs%20for%20K8s%20Pods%20within%20Tanzu&amp;summary=Controlling%20the%20egress%20IPs%20for%20K8s%20Pods%20within%20Tanzu%2c%20by%20Steven%20Schramm%0a%0a%3cnil%3e%0a" target="_blank" rel="noopener" aria-label="Share on LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none"
   stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
   <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
   <rect x="2" y="9" width="4" height="12"></rect>
   <circle cx="4" cy="4" r="2"></circle>
</svg></a>
        </li>
        <li>
            <a href="#" onclick="linkShare(&#34;Controlling the egress IPs for K8s Pods within Tanzu&#34;,&#34;https://sdn-techtalk.com/posts/antrea-egress/&#34;,&#34;Controlling the egress IPs for K8s Pods within Tanzu, by Steven Schramm\n\n\u003cnil\u003e\n&#34;); return false;" target="_self" rel="noopener" aria-label="Copy Link"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-copy">
      <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
      <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
   </svg></a>
        </li>
    </ul>
</div><button id="menu-btn" class="hdr-btn" title=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
   </svg></button>
			</div>
		</div>
	</header>
	<div id="mobile-menu" class="animated fast">
		<ul>
			<li><a href="https://sdn-techtalk.com/posts/">Posts</a></li>
			<li><a href="https://sdn-techtalk.com/about/">About Me</a></li>
		</ul>
	</div>


	<main class="site-main section-inner animated fadeIn faster"><article class="thin">
			<header class="post-header">
				<div class="post-meta"><span>Dec 21, 2024</span></div>
				<h1>Controlling the egress IPs for K8s Pods within Tanzu</h1>
			</header>
			<div class="post-info"><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
   stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-feather">
   <path d="M20.24 12.24a6 6 0 0 0-8.49-8.49L5 10.5V19h8.5z"></path>
   <line x1="16" y1="8" x2="2" y2="22"></line>
   <line x1="17.5" y1="15" x2="9" y2="15"></line>
</svg><a href="/about/" target="_blank">Steven Schramm</a></p>
<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon">
      <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path>
      <line x1="7" y1="7" x2="7" y2="7"></line>
   </svg><span class="tag"><a href="https://sdn-techtalk.com/tags/homelab">homelab</a></span><span class="tag"><a href="https://sdn-techtalk.com/tags/nested-lab">nested lab</a></span><span class="tag"><a href="https://sdn-techtalk.com/tags/vmware">vmware</a></span><span class="tag"><a href="https://sdn-techtalk.com/tags/esxi">esxi</a></span><span class="tag"><a href="https://sdn-techtalk.com/tags/network">network</a></span><span class="tag"><a href="https://sdn-techtalk.com/tags/tanzu">tanzu</a></span><span class="tag"><a href="https://sdn-techtalk.com/tags/antrea">antrea</a></span><span class="tag"><a href="https://sdn-techtalk.com/tags/nsx">nsx</a></span></p>
<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
      <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
      <polyline points="14 2 14 8 20 8"></polyline>
      <line x1="16" y1="13" x2="8" y2="13"></line>
      <line x1="16" y1="17" x2="8" y2="17"></line>
      <polyline points="10 9 9 9 8 9"></polyline>
   </svg>1919 Words
     Words // ReadTime
    
    
    
    8 Minutes, 43 Seconds</p>
<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
      <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
      <line x1="16" y1="2" x2="16" y2="6"></line>
      <line x1="8" y1="2" x2="8" y2="6"></line>
      <line x1="3" y1="10" x2="21" y2="10"></line>
   </svg>2024-12-21 19:54 &#43;0100</p></div>
			<hr class="post-end">
			<div class="content">
				<h2 id="introduction">Introduction<a href="#introduction" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>The integration of K8s platforms within a enterprise network environment is always a challenge regarding security and routing of certain K8s pods. Multiple pods are running in the same K8s cluster but might have different functinalities and requirements to access services outside of the K8s platform.
What should you do, if you are unable to assign a specific outgoing IP to a specific pod or group of pods? In this case you are forced to treat all the pods within a specific K8s cluster the same way and create firewall rules based on this.
That does mean all the different pods within a K8s cluster will have network permissions and it is very challenging to implement further isolation.</p>
<p>For legacy applications which were deployed on physical servers or specific virtual servers, each of those servers got a specific IP assigned. Based on this it was possible to create specific firewall rules for teh specific servers based on the different requirements to access other services or beeing accessed from other systems.
This brought some isolation and overall security to the enterprise networks and is also a demand to satisfy the guidelines of some specific authorities like BAFIN or based on DORA.</p>
<p>Why is this now a challenge within K8s? K8s is an additinal layer of abstraction to increase the efficiency to deploy different services on a single server. The layer of K8s will be installed ontop of an operationg system, which is either installed on a physical or virtual server.
Since the most enterprised does have a high ratio of virtualisation, it is more likli that K8s is deployed ontop of VM or bunch of VMs to build a K8s cluster. Each of those VMs is now called K8s worker or master and will run one or pultiple K8s services. In this case all the different VMs still have a specific IP adress that can be used in firewall rules, but the K8s services are getting deployed ontop of thoese VMs.
That means multiple K8s services are sharing the same group of VMs and if a K8s service wants to reach a different service outside of K8s it will leave the K8s worker or master by using the IP of those VMs.</p>
<p>But in K8s there are multiple solutions for this challenge. I would like to tell you about one solution that can be used in the K8s platform by VMware Tanzu (VKS) and requires the K8s network plugin (CNI) &ldquo;Antrea&rdquo;.
The specific feature is called &ldquo;Antrea Engress&rdquo;.</p>
<p>I will not show how to deploy the Tanzu supervisor cluster, but how to create a K8s guest cluster with the required features enabled and how to assign specific egress IPs to specific K8s pods.</p>
<h2 id="lab-environment">Lab environment<a href="#lab-environment" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>For the tests I worked with the following lab environment.</p>
<ul>
<li>Three nested ESXi hosts of version 8.0.3, 24022510</li>
<li>vCenter server of version 8.0.3, 24022515</li>
<li>NSX Datacenter version 4.2.1.1</li>
<li>Tanzu supervisor cluster integrated with NSX Datacenter</li>
</ul>
<h2 id="requirements">Requirements<a href="#requirements" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>My tests are based on assigning seperate egress subnets to different K8s pods within the same K8s cluster. To implement this solution the following versions are required.</p>
<ul>
<li>TKr 1.31</li>
<li>TKG Services 3.2.0</li>
</ul>
<p>Further it is required to deploy Tanzu with NSX as networking layer and using Antrea as CNI.
For NSX a specific feature called &ldquo;Child Segments&rdquo; will be used to get this implementation done, so take care you current NSX version is supporting this feature.</p>
<p>The next requirement is optional, but influence the overall configuration that needs to be done manually.
If you are using NAT mode, you need to manually create NoSNAT rules for the NSX child segments that will be cerated within the process. If NAT mode is disabled, this manuall configuration is not required.</p>
<h2 id="nsx-child-segments">NSX child segments<a href="#nsx-child-segments" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>Before digging into the steps how to assign seperate egress subnets to different K8s pods, I will shortly describe whar NSX child segments are.
If you are not aware of NSX and the routers within NSX, you need to familiarize with that first. Therefore I would recommend the NSX reference design guide <a href="https://community.broadcom.com/viewdocument/nsx-reference-design-guide-42-v10?CommunityKey=b76535ef-c5a2-474d-8270-3e83685f020e&amp;tab=librarydocuments">https://community.broadcom.com/viewdocument/nsx-reference-design-guide-42-v10?CommunityKey=b76535ef-c5a2-474d-8270-3e83685f020e&amp;tab=librarydocuments</a>.</p>
<p>NSX child segents are based on a parent segment which is connected to a T1 router. The child segment it self can be either connected to the same or a different T1 router than the parent segment.
The connection between the parent and the child segment will be done by a VLAN tag and two child segments cannot share the same VLAN tag.</p>
<p>This feature is currently not available in the UI and those child segments can be wither created through the NSX API or the Antrea NSX control application.</p>
<p>Based on this feature different pods can be assigned to different subnets without the requirement of assigning different portgroups to the K8s worker and master nodes. Those subnets also does not interfere withe the reserved IPs for the assigned namespace CIDR of the K8s clusters.</p>
<p>More information of the child sgements can be discoverd in the documentation of Broadcom. <a href="https://techdocs.broadcom.com/us/en/vmware-cis/nsx/vmware-nsx/4-2/administration-guide/segments/creating-a-child-segment.html">https://techdocs.broadcom.com/us/en/vmware-cis/nsx/vmware-nsx/4-2/administration-guide/segments/creating-a-child-segment.html</a></p>
<h2 id="prepering-the-k8s-guest-cluster">Prepering the K8s guest cluster<a href="#prepering-the-k8s-guest-cluster" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>Before creating the K8s cluster intself, it is required to create the Antrea configuration to enable the feature &ldquo;EgressSeperateSubnet&rdquo;.
The following example shows an example configuration. This configuration does not negate the default settings of Antrea, which are not mentioned in this YAML-File.
I also added the NSX integration in my configuration to integrate the NSX Distributed Firewall with Antrea, but this will not be further discussed in this article. Further the feature of &ldquo;NodePortLocal&rdquo; is also not required for the assignment of dedicated egress subnets.</p>
<p>The name of the K8s object &ldquo;AntreaConfig&rdquo; is very important and should match the prefix <!-- raw HTML omitted -->-antrea-package, otherwhise the Antrea config will not be assigned. Further this configuration needs to be added before the K8s cluster is created.
It is also possible to adjust the configuration after the deployment of the K8s cluster, but this is much more complex and required a reboot of the K8s nodes.</p>
<pre tabindex="0"><code>apiVersion: cni.tanzu.vmware.com/v1alpha1
kind: AntreaConfig
metadata:
 name: tkc-test01-antrea-package #prefix required
 namespace: test
spec:
  antrea:
    config:
      featureGates:
        EgressSeparateSubnet: true
        NodePortLocal: true
  antreaNSX:
    enable: true #false by default
</code></pre><p>After the configuration is applied, you can start with the K8s cluster deployment. Following my example K8s cluster YAML-File.</p>
<pre tabindex="0"><code>apiVersion: run.tanzu.vmware.com/v1alpha3
kind: TanzuKubernetesCluster
metadata:
  name: tkc-test01
  namespace: test
spec:
  topology:
    controlPlane:
      replicas: 1
      vmClass: best-effort-medium
      storageClass: labnfs
      tkr:
        reference:
          #name: v1.29.4---vmware.3-fips.1-tkg.1
          name: v1.31.1---vmware.2-fips-vkr.2
    nodePools:
    - replicas: 2
      name: worker
      vmClass: best-effort-medium
      storageClass: labnfs
  settings:
    network:
      pods:
        cidrBlocks: [&#34;192.168.0.0/16&#34;]
      services:
        cidrBlocks: [&#34;10.196.0.0/12&#34;]
</code></pre><h2 id="creating-test-k8s-deployments">Creating test K8s deployments<a href="#creating-test-k8s-deployments" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>As a next step I created two namespaces to assign two different egress subnets later on.</p>
<ul>
<li>Namespace 1: prod</li>
<li>Namespace 2: staging</li>
</ul>
<p>In each of those namespaces I created a test deployment based on the follwoing YAML-File.</p>
<pre tabindex="0"><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: nginx
          image: nginx:latest
</code></pre><p>The commands used for this deployment are shown below.</p>
<p>Creating namespaces:</p>
<pre tabindex="0"><code>kubectl create ns &lt;namespace name&gt;
</code></pre><p>Creating deployment:</p>
<pre tabindex="0"><code>kubectl apply -f web.yaml -n &lt;namespace name&gt;
</code></pre><p>In addition to the K8s deployments I deployed a docker container outside my NSX environment based on the following steps.</p>
<ol>
<li>Create folder your Dockerfile and create the Dockerfile</li>
</ol>
<pre tabindex="0"><code>FROM tiangolo/uvicorn-gunicorn-fastapi:python3.9
COPY ./app /app
</code></pre><ol start="2">
<li>Create a subolder called &ldquo;app&rdquo;</li>
<li>Create a file with the name &ldquo;main.py&rdquo; in the subfolder &ldquo;app&rdquo; with the content below.</li>
</ol>
<pre tabindex="0"><code>from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse

app = FastAPI()

@app.get(&#34;/&#34;, response_class=HTMLResponse)
async def index(request: Request):
    client_host = request.client.host
    return &#34;Requester IP: &#34; + client_host + &#34;\n&#34;
</code></pre><ol start="4">
<li>Go back to the folder where the Dockerfile is located and build the docker image</li>
</ol>
<pre tabindex="0"><code>docker build -t showfastip .
</code></pre><ol start="5">
<li>Run the docker imaged with a choosen port, which is not in use. I decided to use external port 88</li>
</ol>
<pre tabindex="0"><code>docker run -d -p 88:80 showfastip
</code></pre><h2 id="create-child-segments">Create child segments<a href="#create-child-segments" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>I decided to create the child segments with the commandline tool &ldquo;antreansxctl&rdquo;. This commandline tool can be either downloaded to your local PC or you can connect to the Antrea pod of your K8s cluster to execute the desired command.</p>

    <aside class="admonition warning">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-alert-circle">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="8" x2="12" y2="12"></line>
      <line x1="12" y1="16" x2="12.01" y2="16"></line>
   </svg></div><b>It&#39;s important to note that the antreansxctl commandline tool is provided in the internetworking pod</b>
        </div>
        <div class="admonition-content">This Pod will be deployed, if you enable the feature antreaNSX show above in the AntreaConfig example.</div>
    </aside>
<p>A connection to the internetworking pod can be done by the kubectl command below.</p>
<pre tabindex="0"><code>kubectl exec -n vmware-system-antrea interworking-5bdd8f5b5f-wd6wg -c mp-adapter -it -- bash
</code></pre><p>The command to create a child segment requires the follwoing information.</p>
<ul>
<li>Path of the parent segment: can be gathered via NSX API by the code &ldquo;curl &ndash;location &ndash;request GET &lsquo;<a href="https://nsx.lab.home/policy/api/v1/infra/segments/'">https://nsx.lab.home/policy/api/v1/infra/segments/'</a> &ndash;header &lsquo;Content-Type: application/json&rsquo; &ndash;header &lsquo;Authorization: Basic <!-- raw HTML omitted -->&rsquo;&rdquo;</li>
<li>NSX Manager IP</li>
<li>NSX Manager User and Password</li>
<li>VLAN ID used to tag the child segment (this VLAN ID will not be visible outside of NSX and will to overlap with any VLAN on the underlay network)</li>
<li>CIDR and Gateway IP of the child segmnet (the gateway IP will be created on the T1 router used for the child segment, which is the same T1 as for the parent T1 in this case)</li>
</ul>
<p>Command description:</p>
<pre tabindex="0"><code>antreansxctl child-segment-create --nsx-managers=&lt;NSX Manager IP&gt; --user=admin --password=&#39;&lt;NSX Manager admin password&gt;&#39; --cidr=&#34;&lt;CIDR child segment&gt;&#34; --gateway=&#34;&lt;Gateway IP of child segment&gt;&#34; --parent=&#34;&lt;path of parent segment&gt;&#34; --vlan=&lt;vlan id&gt; &lt;name of child segment&gt;
</code></pre><p>An example of this command is shown below.</p>
<pre tabindex="0"><code>antreansxctl child-segment-create --nsx-managers=10.0.1.12 --user=admin --password=&#39;VMware1!VMware1!&#39; --cidr=&#34;10.100.5.0/25&#34; --gateway=&#34;10.100.5.1&#34; --parent=&#34;/infra/segments/vnet_60f55536-121d-40d0-ac48-4cb976decf3d_0&#34; --vlan=110 egress-prod
</code></pre><p>I executed the same command for staging with some other parameters.</p>
<pre tabindex="0"><code>antreansxctl child-segment-create --nsx-managers=10.0.1.12 --user=admin --password=&#39;VMware1!VMware1!&#39; --cidr=&#34;10.100.5.128/30&#34; --gateway=&#34;10.100.5.129&#34; --parent=&#34;/infra/segments/vnet_60f55536-121d-40d0-ac48-4cb976decf3d_0&#34; --vlan=120 egress-staging
</code></pre><p>The two child segments are now visible in the NSX UI as shown in the following picture.
<img src="childsegmentsui.png" alt="Childsegents"></p>
<h2 id="create-and-assign-antrea-external-ip-pools">Create and assign antrea external ip pools<a href="#create-and-assign-antrea-external-ip-pools" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>I created one external IP pool for staging and one for prod.</p>
<p>Prod external IP Pool YAML-File:</p>
<pre tabindex="0"><code>apiVersion: crd.antrea.io/v1beta1
kind: ExternalIPPool
metadata:
  name: prod-external-ip-pool
spec:
  ipRanges:
  - start: 10.100.5.10
    end: 10.100.5.20
  subnetInfo:
    gateway: 10.100.5.1
    prefixLength: 25
    vlan: 110
  nodeSelector: {}
</code></pre><p>Staging external IP pool YAML-File:</p>
<pre tabindex="0"><code>apiVersion: crd.antrea.io/v1beta1
kind: ExternalIPPool
metadata:
  name: staging-external-ip-pool
spec:
  ipRanges:
  - start: 10.100.5.130
    end: 10.100.5.130
  subnetInfo:
    gateway: 10.100.5.129
    prefixLength: 30
    vlan: 120
  nodeSelector: {}
</code></pre><p>Creating the two IP pools:</p>
<pre tabindex="0"><code>kubectl apply -f antrea-externalippool-prod.yaml
kubectl apply -f antrea-externalippool-staging.yaml
</code></pre><p>YAML-Files for assignment of prod IP pool to prod web deployment:</p>
<pre tabindex="0"><code>apiVersion: crd.antrea.io/v1beta1
kind: Egress
metadata:
  name: egress-prod-web
spec:
  appliedTo:
    namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: prod
    podSelector:
      matchLabels:
        app: web
  externalIPPool: prod-external-ip-pool
</code></pre><p>YAML-Files for assignment of staging IP pool to prod web deployment:</p>
<pre tabindex="0"><code>apiVersion: crd.antrea.io/v1beta1
kind: Egress
metadata:
  name: egress-staging-web
spec:
  appliedTo:
    namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: staging
    podSelector:
      matchLabels:
        app: web
  externalIPPool: staging-external-ip-pool
</code></pre><p>Assigning  the external IP pools:</p>
<pre tabindex="0"><code>kubectl apply -f egress-prod-web.yaml -n prod
kubectl apply -f egress-staging-web.yaml -n staging
</code></pre><p>The following command shows the assigned IPs of the two IP pools.</p>
<pre tabindex="0"><code>kubectl get egress
NAME                 EGRESSIP       AGE   NODE
egress-prod-web      10.100.5.10    42s   tkc-test01-hsqsg-vmm2w
egress-staging-web   10.100.5.130   28s   tkc-test01-worker-s9s27-qqhrg-z6jd5
</code></pre><h2 id="create-nosnat-rule">Create noSNAT rule<a href="#create-nosnat-rule" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>As already mentioned it is required to manually configure a noSNAT rule for the external IP pool subnets, if NAT mode is enabled for the used vSphere Namespace where the K8s cluster is deployed.
The noSNAT rule needs to be configured on the T1 router where the child segments are connected as shown in the picture below.</p>
<p><img src="nosnat.png" alt="noSNAT"></p>
<h2 id="testing-if-egress-ip-pool-assigment-is-working">Testing if egress ip pool assigment is working<a href="#testing-if-egress-ip-pool-assigment-is-working" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>The following tests are showing an access from the web pod in both namespaces and which external IP will be used to access the webserver deployed in dockers as mentioned earlier.
The Requester IP is the egress IP visible for the target web serverrunning in docker.</p>
<pre tabindex="0"><code>stevenschramm@Stevens-MacBook-Pro homelab % kubectl -n prod exec web-54d6b8f598-7ddqv -- curl -s 192.168.178.222:88
Requester IP: 10.100.5.10
stevenschramm@Stevens-MacBook-Pro homelab % kubectl -n staging exec web-54d6b8f598-slnm8 -- curl -s 192.168.178.222:88
Requester IP: 10.100.5.130
</code></pre>
			</div>
			

		</article>
		<div class="post-nav thin">
		</div>
		<div id="comments" class="thin"></div>
	</main>

<footer id="site-footer" class="section-inner thin animated fadeIn faster">
	<p>
		&copy; 2024 <a href="https://sdn-techtalk.com/">Steven Schramm</a>
		&#183; COPYRIGHT Steven Schramm
		&#183; <a href="https://sdn-techtalk.com/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
   stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss">
   <path d="M4 11a9 9 0 0 1 9 9"></path>
   <path d="M4 4a16 16 0 0 1 16 16"></path>
   <circle cx="5" cy="19" r="1"></circle>
</svg></a></p>

</footer>
<script async src="https://sdn-techtalk.com/js/bundle.min.c7c384e4d29d192bbac6811ae4660bb01767194a5bea56baca77e8260f93ea16.js" integrity="sha256-x8OE5NKdGSu6xoEa5GYLsBdnGUpb6la6ynfoJg+T6hY=" crossorigin="anonymous"></script><script async src="https://sdn-techtalk.com/js/link-share.min.24409a4f6e5537d70ffc55ec8f9192208d718678cb8638585342423020b37f39.js" integrity="sha256-JECaT25VN9cP/FXsj5GSII1xhnjLhjhYU0JCMCCzfzk=" crossorigin="anonymous"></script>
</body>

</html>
