<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on SDN-Techtalk | Steven Schramm</title>
		<link>https://sdn-techtalk.com/posts/</link>
		<description>Recent content in Posts on SDN-Techtalk | Steven Schramm</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>Steven Schramm</copyright>
		<lastBuildDate>Thu, 02 Jan 2025 12:00:00 +0100</lastBuildDate>
		<atom:link href="https://sdn-techtalk.com/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Improving NSX Datacenter TEP performance and availability - Multi-TEP and TEP Group High Availability</title>
			<link>https://sdn-techtalk.com/posts/multitep-ha/</link>
			<pubDate>Thu, 02 Jan 2025 12:00:00 +0100</pubDate>
			
			<guid>https://sdn-techtalk.com/posts/multitep-ha/</guid>
			<description><![CDATA[%!s(<nil>)]]></description>
			<content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Some of you are using NSX for many years already and are aware of the different changes and improvements implemented in the last years.
I personally started with NSX in version 2.3 and one of the first important improvements I recognized is &ldquo;MultiTEP&rdquo; for edge nodes from type VM.  It was released with NSX 2.5 and officially added to the reference design guide.</p>
<p>By the way: The reference design guide is still a great resource to learn the design pricipals for NSX implementaions. This is especially interesting for those who might be new to NSX.</p>
<p>Over the years some additional enhancements were implemeted like the enhanced datapath, ECMP optimisations and many more.
But one major problem in case of specific failover scenarios was still in place! This problem comes into place, if the physical uplinks of the ESXi hosts and the baremetal edges or the virtual network interfaces of the edge VMs are up, but the layer2 or layer3 connectivity of the TEP interfaces is broken.
In this situation the TEP interfaces do not failover, since the assigned BFD sessions are validating the link states only.
That means a failover of a TEP interface was just triggered by a failed connectivity between the physical switch and the connected port of the host transport nodes. For edges this will generally not happen and therefore a failover will not be triggered, if the connectivity is broken and the virtual cable still connected.</p>
<p>This behavior is finally changed in NSX 4.1 by implementing Multi-TEP High Availability and supplemented by TEP group HA, which is available for edge nodes since NSX 4.2.1.
In addition the TEP group feature improves the load sharing for the North/ South traffic of overlay segments or all overlay traffic sent over edge nodes by using a service router. The load sharing feature of TEP groups is also available for edges in NSX 4.2.0, just the HA addition was implemented a bit later in 4.2.1. You might wonder why I am talking about load sharing as a new feature, but this was previously implemented on per segment basis and not per flow.</p>
<p>Within this blog post I will talk about the HA and load sharing behavior and compare the behavior with and without those features. The results will be validated by the following test cases.</p>
<ul>
<li>Inspecting packet capture created on the two fp-eth interfaces used for the edge node after implementing the new features (Check section <a href="https://sdn-techtalk.com/posts/multitep-ha/#what-is-multi-tep-and-tep-group-ha">&ldquo;What is Multi-TEP and TEP Group HA?&rdquo;</a>)</li>
<li>Breaking Layer2 communication of an ESXi host by manipulating the assigned VLAN IDs before implementing the new features (Check section <a href="https://sdn-techtalk.com/posts/multitep-ha/#test-case-execution-before-applying-the-changes">&ldquo;Test case execution before applying the changes&rdquo;</a>)</li>
<li>Breaking Layer2 communication of an egde node by manipulating the assigned VLAN IDs before implementing the new features (Check section <a href="https://sdn-techtalk.com/posts/multitep-ha/#test-case-execution-before-applying-the-changes">&ldquo;Test case execution before applying the changes&rdquo;</a>)</li>
<li>Breaking Layer2 communication of an ESXi host by manipulating the assigned VLAN IDs after implementing the new features (Check section <a href="https://sdn-techtalk.com/posts/multitep-ha/#test-case-execution-after-the-changes-are-applied">&ldquo;Test case execution after applying the changes&rdquo;</a>)</li>
<li>Breaking Layer2 communication of an egde node by manipulating the assigned VLAN IDs after implementing the new features (Check section <a href="https://sdn-techtalk.com/posts/multitep-ha/#test-case-execution-after-the-changes-are-applied">&ldquo;Test case execution after applying the changes&rdquo;</a>)</li>
</ul>
<p>Before and while implementing those test cases, I will ping a test VM to show the behavior of the communication.</p>
<h2 id="lab-environment">Lab environment</h2>
<p>For the tests I worked with the following lab environment.</p>
<ul>
<li>Three nested ESXi hosts of version 8.0.3, 24022510</li>
<li>vCenter server of version 8.0.3, 24022515</li>
<li>NSX Datacenter version 4.2.1.1</li>
<li>Alpine test VM</li>
</ul>
<h2 id="what-is-multi-tep-and-tep-group-ha">What is Multi-TEP and TEP Group HA?</h2>
<p>Multi-TEP HA is a feature implemented in NSX Datacenter 4.1 and optimizes the behavior of TEP failover. Starting with NSX Datacenter 4.1 it was applicable for host transport nodes only, not for edge transport nodes.
Without Multi-TEP HA a layer2 or layer3 outage did not cause an failover of a TEP on any type of NSX transport nodes, if the link state is kept online. The missing failover of the affected TEP caused an outage for all worklaods using this TEP interface.
Based on the loadsharing alogorithm &ldquo;Load Balance source&rdquo; this will cause approx. 50% of the whole workload located on the affected transport nodes.</p>
<p>Multi-TEP HA solves this challenge by implementing BFD sessions actively sending keepalive messages between the different transport nodes. Those keepalive messages will trigger a failover in case the corresponding BFD sessions switching the state from &ldquo;UP&rdquo; to &ldquo;DOWN&rdquo;.</p>
<p>As an example the following drawing shows the connection between two different host transport nodes using two TEP interfaces each. This drawing also shows the base scenario used to execute the upcoming failover scenarios.</p>
<p><a href="notepha-before-failover.png"><img src="notepha-before-failover.png" alt="notepha-before-failover"></a></p>
<p>Based on the BFD sessions validating only the link state instead of sending real keepalive messages, the following drwawing shows the two possible failover behaviors without Multi-TEP HA.
The first situation where the link changes his state from &ldquo;UP&rdquo; to &ldquo;DOWN&rdquo; and the second situation where the layer2 communication is broken, but the link is still in state &ldquo;UP&rdquo;.
As you can see in the first situation the VM will be still available, but for the second situation the VM becomes unavailable.</p>
<p><a href="notepha-failover.png"><img src="notepha-failover.png" alt="notepha-failover"></a></p>
<p>After the implementation of Multi-TEP HA the second situation is covered as well as the first situation. The result is shown in the drawing below.</p>
<p><a href="tepha-failover.png"><img src="tepha-failover.png" alt="tepha-failover"></a></p>
<p>The next scenario shows a failover for edge transport nodes which are using multiple TEP interfaces. For edge transport nodes this requires the configuration of TEP Groups which are available since NSX 4.2 and TEP Group HA introduced with NSX 4.2.1.
A TEP Group defines a group of multiple TEP interfaces, which will be used instead of the dedicated fastpath interfaces like &ldquo;fp-eth0&rdquo; available on the edge transport nodes from type VM.
Without the enhancement of Multi-TEP HA it improves the load sharing behavior for communication sent of edge transport nodes. If TEP Groups are not implemented each overlay segment is explicitly bound with a single fastpath interface.
This can be validated with the command &ldquo;get logical-switch&rdquo; or &ldquo;get segments&rdquo;, if you are connected to the nsxcli of an edge transport node. The following output shows an example output of my lab environment.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">UUID                                   VNI          ENCAP    DEVICE       NAME                                               GLOBAL_VNI(FED)
</span></span><span class="line"><span class="cl">3465eadb-9f77-4f33-978c-c70eb5746195   70657        GENEVE   fp-eth1      avi-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-0
</span></span><span class="line"><span class="cl">46299707-dc54-46e8-8e64-aee000f47789   65536        GENEVE   fp-eth0      multicast-transit-bp-bf5d6838-365d-4896-a86d-ff038f3aefe0
</span></span><span class="line"><span class="cl">40ae9a74-03f2-41c7-b8c7-a8bcb70461ca   68609        GENEVE   fp-eth1      seg-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-test-0
</span></span><span class="line"><span class="cl">ee681ecb-224c-48a6-b5d0-11b5a713d3d4   72706        GENEVE   fp-eth0      transit-rl-1af22a37-499a-40af-9f38-4e399ca9b3df
</span></span><span class="line"><span class="cl">c9390ed0-c587-4214-b26e-26705d4cd2f0   74753        GENEVE   fp-eth1      seg-preseveclientip-backend
</span></span><span class="line"><span class="cl">7439a8de-2137-4ce6-a492-d7e75cec8f21   74752        GENEVE   fp-eth1      inter-sr-routing-bp-t0-test
</span></span><span class="line"><span class="cl">0fb2a9a8-2042-483f-8f50-3c9a7e7acfae   65538        GENEVE   fp-eth1      transit-bp-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b
</span></span><span class="line"><span class="cl">e1c55b2c-6b32-4166-b6b7-2fdc125dabdc   65539        GENEVE   fp-eth1      vnet-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-test-tkc-test01-mz-820c6-0
</span></span><span class="line"><span class="cl">d47b2c28-d25e-4224-a4b8-6d300ae3baa5   68611        GENEVE   fp-eth1      transit-bp-t1_c21033c0-3741-43c7-a4bb-11f6b3ef0a7f_rtr
</span></span><span class="line"><span class="cl">5c8d0a5e-e1dc-4474-b27a-9f0e8c03ca25   65537        GENEVE   fp-eth1      transit-rl-194b6bb8-a56e-4c3d-8236-9687c93c64d5
</span></span><span class="line"><span class="cl">64ae82a1-769c-4f78-83e4-a42978bcd07c   71682        GENEVE   fp-eth1      avi-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-test-0
</span></span><span class="line"><span class="cl">4e3820e9-8e3d-40aa-b21b-f7b9c02079b8   72704        GENEVE   fp-eth1      avi-mgmt
</span></span><span class="line"><span class="cl">4fbe907a-e30d-4a86-989d-ae390fef74fc   72705        GENEVE   fp-eth1      avi-data
</span></span><span class="line"><span class="cl">c26f82d1-987f-4d36-b99b-a6efe3b01789   69633        GENEVE   fp-eth1      transit-bp-t1-preserveclientip-test
</span></span><span class="line"><span class="cl">6fe806ec-64cc-47b8-8451-84ffeb65fecd   65542        GENEVE   fp-eth0      egress-staging
</span></span><span class="line"><span class="cl">d509b42e-5db6-4458-9543-17a582fe1e40   65541        GENEVE   fp-eth1      seg-preserveclientip-sedata
</span></span><span class="line"><span class="cl">9ac22341-e348-4d76-8444-456d5d179e3a   70656        GENEVE   fp-eth0      transit-rl-6f9eb35f-7778-4eca-ba9d-b7dd4af26800
</span></span><span class="line"><span class="cl">91384425-136a-48c6-8245-fa370f02112c   68612        GENEVE   fp-eth1      transit-rl-7ea6dc1b-bd41-4028-96ab-8c351c1fee67
</span></span><span class="line"><span class="cl">6f3b4dc3-90a4-4dbd-97f5-8f7f4c00c3af   67584        GENEVE   fp-eth0      transit-rl-a879bdaf-37d8-4dda-801d-76fa5bf0c670
</span></span><span class="line"><span class="cl">106aedc8-08b2-48a9-8f34-5d590faf5f99   69632        GENEVE   fp-eth0      vm-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-vif-default-network-0
</span></span><span class="line"><span class="cl">f8a9f3b1-827d-4de0-8b80-081220566a07   71683        GENEVE   fp-eth0      egress-prod
</span></span><span class="line"><span class="cl">a1b4f3f7-ce15-4e92-876a-591abc15dddc   68610        GENEVE   fp-eth0      avi-data2
</span></span><span class="line"><span class="cl">40069c22-3a20-476c-bc16-113892c29eb7   73728        GENEVE   fp-eth0      transit-rl-8ed6fb22-d842-42b6-b034-c7dd2e053f02
</span></span><span class="line"><span class="cl">dcbd8196-5550-466e-9bae-2c4f08476386   71681        GENEVE   fp-eth1      seg-tanzu-infra
</span></span><span class="line"><span class="cl">b7f36c3d-8415-4904-9e6d-1820dfeb106f   71680        GENEVE   fp-eth0      transit-bp-t0-test
</span></span></code></pre></div><p>In this example the whole communication of an overlay segment will be sent over the shown fastpath interface within the privous output. This is the case for each packet sent through the edge transport nodes.
That means you will not have any load sharing, if you are using a single segment. Loadsharing is just applicable, if you are using mutiple segments and those segments are mapped to different fastpath interfaces.</p>
<p>After the feature of TEP groups is enabled, the output of the segments will be as shown below. But this time it is required to use the command &ldquo;get logical-switch&rdquo;, since &ldquo;get segments&rdquo; does not show any device for the overlay segments, if TEP groups are enabled.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">UUID                                   VNI          ENCAP    TEP_GROUP    NAME                                               GLOBAL_VNI(FED)
</span></span><span class="line"><span class="cl">3465eadb-9f77-4f33-978c-c70eb5746195   70657        GENEVE   285696       avi-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-0
</span></span><span class="line"><span class="cl">46299707-dc54-46e8-8e64-aee000f47789   65536        GENEVE   285696       multicast-transit-bp-bf5d6838-365d-4896-a86d-ff038f3aefe0
</span></span><span class="line"><span class="cl">40ae9a74-03f2-41c7-b8c7-a8bcb70461ca   68609        GENEVE   285696       seg-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-test-0
</span></span><span class="line"><span class="cl">ee681ecb-224c-48a6-b5d0-11b5a713d3d4   72706        GENEVE   285696       transit-rl-1af22a37-499a-40af-9f38-4e399ca9b3df
</span></span><span class="line"><span class="cl">c9390ed0-c587-4214-b26e-26705d4cd2f0   74753        GENEVE   285696       seg-preseveclientip-backend
</span></span><span class="line"><span class="cl">7439a8de-2137-4ce6-a492-d7e75cec8f21   74752        GENEVE   285696       inter-sr-routing-bp-t0-test
</span></span><span class="line"><span class="cl">0fb2a9a8-2042-483f-8f50-3c9a7e7acfae   65538        GENEVE   285696       transit-bp-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b
</span></span><span class="line"><span class="cl">e1c55b2c-6b32-4166-b6b7-2fdc125dabdc   65539        GENEVE   285696       vnet-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-test-tkc-test01-mz-820c6-0
</span></span><span class="line"><span class="cl">d47b2c28-d25e-4224-a4b8-6d300ae3baa5   68611        GENEVE   285696       transit-bp-t1_c21033c0-3741-43c7-a4bb-11f6b3ef0a7f_rtr
</span></span><span class="line"><span class="cl">5c8d0a5e-e1dc-4474-b27a-9f0e8c03ca25   65537        GENEVE   285696       transit-rl-194b6bb8-a56e-4c3d-8236-9687c93c64d5
</span></span><span class="line"><span class="cl">64ae82a1-769c-4f78-83e4-a42978bcd07c   71682        GENEVE   285696       avi-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-test-0
</span></span><span class="line"><span class="cl">4e3820e9-8e3d-40aa-b21b-f7b9c02079b8   72704        GENEVE   285696       avi-mgmt
</span></span><span class="line"><span class="cl">4fbe907a-e30d-4a86-989d-ae390fef74fc   72705        GENEVE   285696       avi-data
</span></span><span class="line"><span class="cl">c26f82d1-987f-4d36-b99b-a6efe3b01789   69633        GENEVE   285696       transit-bp-t1-preserveclientip-test
</span></span><span class="line"><span class="cl">6fe806ec-64cc-47b8-8451-84ffeb65fecd   65542        GENEVE   285696       egress-staging
</span></span><span class="line"><span class="cl">d509b42e-5db6-4458-9543-17a582fe1e40   65541        GENEVE   285696       seg-preserveclientip-sedata
</span></span><span class="line"><span class="cl">9ac22341-e348-4d76-8444-456d5d179e3a   70656        GENEVE   285696       transit-rl-6f9eb35f-7778-4eca-ba9d-b7dd4af26800
</span></span><span class="line"><span class="cl">91384425-136a-48c6-8245-fa370f02112c   68612        GENEVE   285696       transit-rl-7ea6dc1b-bd41-4028-96ab-8c351c1fee67
</span></span><span class="line"><span class="cl">6f3b4dc3-90a4-4dbd-97f5-8f7f4c00c3af   67584        GENEVE   285696       transit-rl-a879bdaf-37d8-4dda-801d-76fa5bf0c670
</span></span><span class="line"><span class="cl">106aedc8-08b2-48a9-8f34-5d590faf5f99   69632        GENEVE   285696       vm-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-vif-default-network-0
</span></span><span class="line"><span class="cl">f8a9f3b1-827d-4de0-8b80-081220566a07   71683        GENEVE   285696       egress-prod
</span></span><span class="line"><span class="cl">a1b4f3f7-ce15-4e92-876a-591abc15dddc   68610        GENEVE   285696       avi-data2
</span></span><span class="line"><span class="cl">40069c22-3a20-476c-bc16-113892c29eb7   73728        GENEVE   285696       transit-rl-8ed6fb22-d842-42b6-b034-c7dd2e053f02
</span></span><span class="line"><span class="cl">dcbd8196-5550-466e-9bae-2c4f08476386   71681        GENEVE   285696       seg-tanzu-infra
</span></span><span class="line"><span class="cl">b7f36c3d-8415-4904-9e6d-1820dfeb106f   71680        GENEVE   285696       transit-bp-t0-test
</span></span></code></pre></div><p>As you can see in the previous output, the different segments are now mapped with a TEP group instead of a dedicated fastpath interfaces. As soon as this is the case the communication of all overlay segments is load-shared on per flow basis.
Traffic sourced by edge transport nodes is then load-shared across all source edge TEPs and the traffic destined to edge transport nodes is then load-shared across all destination edge TEPs.
As prove of the flow based loadsharing I did a test on a alpine VM connected to the segment &ldquo;seg-preseveclientip-backend&rdquo; by using a iperf session.
The alpine VM acts as iperf client and a server outside of the NSX overlay acts as the iperf server.</p>
<p>Starting the iperf server on the destination:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">iperf3 -s -p 5001
</span></span></code></pre></div><p>Starting iperf on the clinet (alpine VM):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">iperf3 -c &lt;ip of iperf server&gt; -p 5001
</span></span></code></pre></div><p>To track the used fastpath interfaces I started a packet capture on the edge transport nodes from the nsxcli by using the following command.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">set capture session 1 interface fp-eth0 direction dual
</span></span><span class="line"><span class="cl">set capture session 1 interface fp-eth1 direction dual
</span></span><span class="line"><span class="cl">set capture session 1 file tepha.pcap
</span></span></code></pre></div><p>To be able to understand the generated packet capture the following overview shows the two fastpath interfaces of the edge transport node used to forward the North/ South communication. Further the list includes the VM used for the test communication and the required iperf server.</p>
<ul>
<li>fp-eth0:
<ul>
<li>TEP IP: 10.0.2.16</li>
<li>MAC: 00:50:56:8d:bf:e4</li>
</ul>
</li>
<li>fp-eth1:
<ul>
<li>TEP IP: 10.0.2.17</li>
<li>MAC: 00:50:56:8d:df:ce</li>
</ul>
</li>
<li>Alpine VM:
<ul>
<li>Segment: seg-preseveclientip-backend</li>
<li>IP: 10.100.12.2</li>
</ul>
</li>
<li>IPerf Server:
<ul>
<li>IP: 192.168.178.222</li>
</ul>
</li>
</ul>
<p>The following two screenshots are the prove that the same VM connected to the same segment is load-shared over both fastpath interfaces. Please be aware, that the packet capture examples are showing the response packets only.</p>
<p>PCAP output where flow is using fp-eth0:
<a href="pcap-flow-fpeth0.png"><img src="pcap-flow-fpeth0.png" alt="pcap-flow-fpeth0"></a></p>
<p>PCAP output where flow is using fp-eth1:
<a href="pcap-flow-fpeth1.png"><img src="pcap-flow-fpeth1.png" alt="pcap-flow-fpeth1"></a></p>
<p>After upgrading to NSX 4.2.1, the feature of Multi-TEP is automatically enabled for als edge transport node TEP interfaces, if the TEP group feature is enabld.
The following drawing visualizes the mapping of a overlay sgement with and without TEP groups.</p>
<p><a href="tepgroup-before-failover.png"><img src="tepgroup-before-failover.png" alt="tepgroup-before-failover"></a></p>
<p>In the next drawing you can discover the result of a layer2 comminication issue in case you are using TEP groups with Multi-TEP HA compared to a implementation without TEP groups and no Multi-TEP HA as TEP group enhancement.</p>
<p><a href="tepgroup-failover.png"><img src="tepgroup-failover.png" alt="tepgroup-failover"></a></p>
<h2 id="how-to-implement-multi-tep-ha">How to implement Multi-TEP HA?</h2>
<p>To implement Muti-TEP HA the following steps are required.</p>
<ol>
<li>Create a TEP HA host switch profile as shown below.</li>
</ol>
<pre tabindex="0"><code> PUT https://&lt;nsx-policy-manager&gt;/policy/api/v1/infra/host-switch-profiles/vtephaprofile1 
</code></pre><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-JSON" data-lang="JSON"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="nt">&#34;enabled&#34;</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="nt">&#34;failover_timeout&#34;</span><span class="p">:</span><span class="s2">&#34;5&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="nt">&#34;auto_recovery&#34;</span> <span class="p">:</span> <span class="s2">&#34;true&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="nt">&#34;auto_recovery_initial_wait&#34;</span> <span class="p">:</span> <span class="s2">&#34;300&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="nt">&#34;auto_recovery_max_backoff&#34;</span> <span class="p">:</span> <span class="s2">&#34;86400&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;PolicyVtepHAHostSwitchProfile&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="nt">&#34;display_name&#34;</span><span class="p">:</span> <span class="s2">&#34;VtepHAProfile1&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><ol start="2">
<li>Assign the TEP HA profile to an transport node profile. Based on the asignment of the transport node profile, you are able to select the hosts which should get the Multi-TEP HA feature enabled. Do not forget to gather the ID of the transport node profile, otherwhise you are unable to map the TEP HA profile.</li>
</ol>
<pre tabindex="0"><code>PUT https://&lt;nsx-policy-manager&gt;/policy/api/v1/infra/host-transport-node-profiles/&lt;tnp-id&gt; 
</code></pre><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-JSON" data-lang="JSON"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;host_switch_spec&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;host_switches&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;host_switch_name&#34;</span><span class="p">:</span> <span class="s2">&#34;vtepha_vds&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;host_switch_id&#34;</span><span class="p">:</span> <span class="s2">&#34;50 22 ee c4 f6 40 79 8b-0e a4 2b da 6a 4c 36 b3&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;host_switch_type&#34;</span><span class="p">:</span> <span class="s2">&#34;VDS&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;host_switch_mode&#34;</span><span class="p">:</span> <span class="s2">&#34;ENS_INTERRUPT&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;host_switch_profile_ids&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">                    <span class="p">{</span>
</span></span><span class="line"><span class="cl">                        <span class="nt">&#34;key&#34;</span><span class="p">:</span> <span class="s2">&#34;UplinkHostSwitchProfile&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="nt">&#34;value&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/host-switch-profiles/b32e6ce6-f1ba-4e31-a2c4-33d550505cdd&#34;</span>
</span></span><span class="line"><span class="cl">                    <span class="p">},</span>
</span></span><span class="line"><span class="cl">                    <span class="p">{</span>
</span></span><span class="line"><span class="cl">                        <span class="nt">&#34;key&#34;</span><span class="p">:</span> <span class="s2">&#34;VtepHAHostSwitchProfile&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="nt">&#34;value&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/host-switch-profiles/vtephaprofile1&#34;</span>
</span></span><span class="line"><span class="cl">                    <span class="p">}</span>
</span></span><span class="line"><span class="cl">                <span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;uplinks&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">                    <span class="p">{</span>
</span></span><span class="line"><span class="cl">                        <span class="nt">&#34;vds_uplink_name&#34;</span><span class="p">:</span> <span class="s2">&#34;Uplink 1&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="nt">&#34;uplink_name&#34;</span><span class="p">:</span> <span class="s2">&#34;uplink-1&#34;</span>
</span></span><span class="line"><span class="cl">                    <span class="p">},</span>
</span></span><span class="line"><span class="cl">                    <span class="p">{</span>
</span></span><span class="line"><span class="cl">                        <span class="nt">&#34;vds_uplink_name&#34;</span><span class="p">:</span> <span class="s2">&#34;Uplink 2&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="nt">&#34;uplink_name&#34;</span><span class="p">:</span> <span class="s2">&#34;uplink-2&#34;</span>
</span></span><span class="line"><span class="cl">                    <span class="p">}</span>
</span></span><span class="line"><span class="cl">                <span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;is_migrate_pnics&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;ip_assignment_spec&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                    <span class="nt">&#34;ip_pool_id&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/ip-pools/v4&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;StaticIpPoolSpec&#34;</span>
</span></span><span class="line"><span class="cl">                <span class="p">},</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;cpu_config&#34;</span><span class="p">:</span> <span class="p">[],</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;transport_zone_endpoints&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">                    <span class="p">{</span>
</span></span><span class="line"><span class="cl">                        <span class="nt">&#34;transport_zone_id&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/sites/default/enforcement-points/default/transport-zones/de47a6b9-fa4c-4bf3-bd75-385859895949&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="nt">&#34;transport_zone_profile_ids&#34;</span><span class="p">:</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">                    <span class="p">}</span>
</span></span><span class="line"><span class="cl">                <span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;not_ready&#34;</span><span class="p">:</span> <span class="kc">false</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;StandardHostSwitchSpec&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;ignore_overridden_hosts&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;PolicyHostTransportNodeProfile&#34;</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span></code></pre></div><ol start="3">
<li>Verify if the TEP HA profile is applied to the transport node profile</li>
</ol>
<pre tabindex="0"><code>GET https://&lt;nsx-manager&gt;/policy/api/v1/infra/host-transport-nodes-profiles/&lt;tnp_id&gt; 
</code></pre><p>Expected output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-JSON" data-lang="JSON"><span class="line"><span class="cl"> <span class="s2">&#34;host_switch_profile_ids&#34;</span><span class="err">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">     <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;key&#34;</span><span class="p">:</span> <span class="s2">&#34;VtepHAHostSwitchProfile&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;value&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/host-switch-profiles/&lt;vtephaprofile1&gt;&#34;</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span><span class="err">,</span>
</span></span></code></pre></div><h2 id="how-to-implement-tep-groups">How to implement TEP Groups?</h2>
<p>The impementation of TEP groups of edge transport nodes can be done by changing a single parameter from the NSX API.</p>
<pre tabindex="0"><code>GET https://&lt;NSX manager&gt;/policy/api/v1/infra/connectivity-global-config
</code></pre><p>Important fragment of the expected output shows the value for the required parameter &ldquo;enable_tep_grouping_on_edge&rdquo; set to &ldquo;false&rdquo;:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-JSON" data-lang="JSON"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="nt">&#34;global_replication_mode_enabled&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;is_inherited&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;site_infos&#34;</span><span class="p">:</span> <span class="p">[],</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;tep_group_config&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;enable_tep_grouping_on_edge&#34;</span><span class="p">:</span> <span class="kc">false</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;GlobalConfig&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;global-config&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;display_name&#34;</span><span class="p">:</span> <span class="s2">&#34;default&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;path&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/global-config&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>With the following API call the parameter should be changed to &ldquo;true&rdquo;, if you want to enable the TEP group feature.</p>
<pre tabindex="0"><code>PUT https://&lt;NSX manager&gt;/policy/api/v1/infra/connectivity-global-config
</code></pre><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-JSON" data-lang="JSON"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="nt">&#34;global_replication_mode_enabled&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;is_inherited&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;site_infos&#34;</span><span class="p">:</span> <span class="p">[],</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;tep_group_config&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;enable_tep_grouping_on_edge&#34;</span><span class="p">:</span> <span class="kc">true</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;GlobalConfig&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;global-config&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;display_name&#34;</span><span class="p">:</span> <span class="s2">&#34;default&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;path&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/global-config&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>To enable Multi-TEP HA it is not required to do anything else. It is just required to have NSX updated to version 4.2.1 or later and TEP groups enabled.</p>
<h2 id="test-cases">Test cases</h2>
<p>The following section describes the tests done to validate the behavior before and after the changes. The validation is done by executing cli commands in the nsxcli and the execution of ICMP messages.</p>
<ul>
<li>Source of ICMP: 192.168.178.222</li>
<li>Destination of ICMP: 10.100.12.2 (Alpine VM connected to segment &ldquo;seg-preseveclientip-backend&rdquo;)</li>
</ul>
<p>As an additional overview for the tests you can see the information for the segement where the alpine is connected in comparison to a second segment.</p>
<p>Output of ESXi host</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">get logical-switch
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">  VNI              Logical Switch UUID              Name
</span></span><span class="line"><span class="cl"> 71680     b7f36c3d-8415-4904-9e6d-1820dfeb106f   transit-bp-t0-test
</span></span><span class="line"><span class="cl"> 74753     c9390ed0-c587-4214-b26e-26705d4cd2f0   seg-preseveclientip-backend
</span></span><span class="line"><span class="cl"> 68611     d47b2c28-d25e-4224-a4b8-6d300ae3baa5   transit-bp-t1_c21033c0-3741-43c7-a4bb-11f6b3ef0a7f_rtr
</span></span><span class="line"><span class="cl"> 70657     3465eadb-9f77-4f33-978c-c70eb5746195   avi-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-0
</span></span><span class="line"><span class="cl"> 69633     c26f82d1-987f-4d36-b99b-a6efe3b01789   transit-bp-t1-preserveclientip-test
</span></span><span class="line"><span class="cl"> 65538     0fb2a9a8-2042-483f-8f50-3c9a7e7acfae   transit-bp-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b
</span></span><span class="line"><span class="cl"> 65541     d509b42e-5db6-4458-9543-17a582fe1e40   seg-preserveclientip-sedata
</span></span><span class="line"><span class="cl"> 65539     e1c55b2c-6b32-4166-b6b7-2fdc125dabdc   vnet-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-test-tkc-test01-mz-820c6-0
</span></span><span class="line"><span class="cl"> 73728     40069c22-3a20-476c-bc16-113892c29eb7   transit-rl-8ed6fb22-d842-42b6-b034-c7dd2e053f02
</span></span><span class="line"><span class="cl"> 71681     dcbd8196-5550-466e-9bae-2c4f08476386   seg-tanzu-infra
</span></span><span class="line"><span class="cl"> 65537     5c8d0a5e-e1dc-4474-b27a-9f0e8c03ca25   transit-rl-194b6bb8-a56e-4c3d-8236-9687c93c64d5
</span></span><span class="line"><span class="cl"> 70656     9ac22341-e348-4d76-8444-456d5d179e3a   transit-rl-6f9eb35f-7778-4eca-ba9d-b7dd4af26800
</span></span><span class="line"><span class="cl"> 71682     64ae82a1-769c-4f78-83e4-a42978bcd07c   avi-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-test-0
</span></span><span class="line"><span class="cl"> 72705     4fbe907a-e30d-4a86-989d-ae390fef74fc   avi-data
</span></span><span class="line"><span class="cl"> 68610     a1b4f3f7-ce15-4e92-876a-591abc15dddc   avi-data2
</span></span><span class="line"><span class="cl"> 72706     ee681ecb-224c-48a6-b5d0-11b5a713d3d4   transit-rl-1af22a37-499a-40af-9f38-4e399ca9b3df
</span></span><span class="line"><span class="cl"> 69632     106aedc8-08b2-48a9-8f34-5d590faf5f99   vm-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-vif-default-network-0
</span></span><span class="line"><span class="cl"> 72704     4e3820e9-8e3d-40aa-b21b-f7b9c02079b8   avi-mgmt
</span></span><span class="line"><span class="cl"> 65542     6fe806ec-64cc-47b8-8451-84ffeb65fecd   egress-staging
</span></span><span class="line"><span class="cl"> 67584     6f3b4dc3-90a4-4dbd-97f5-8f7f4c00c3af   transit-rl-a879bdaf-37d8-4dda-801d-76fa5bf0c670
</span></span><span class="line"><span class="cl"> 68612     91384425-136a-48c6-8245-fa370f02112c   transit-rl-7ea6dc1b-bd41-4028-96ab-8c351c1fee67
</span></span><span class="line"><span class="cl"> 68609     40ae9a74-03f2-41c7-b8c7-a8bcb70461ca   seg-domain-c20:d6f4ac77-09e6-4a22-8d95-3b17ba939c3b-test-0
</span></span><span class="line"><span class="cl"> 71683     f8a9f3b1-827d-4de0-8b80-081220566a07   egress-prod
</span></span></code></pre></div><p>The upcomming command shows the assigned TEP IP for the segment &ldquo;seg-preseveclientip-backend&rdquo;.
Within the output below, the &ldquo;Inner MAC&rdquo; is the MAC of the VM connected to this segment, the &ldquo;Outer MAC&rdquo; is the MAC of the TEP interface and the &ldquo;Outer IP&rdquo; is the TEP IP.
Important is the entry &ldquo;LCP Local Entry&rdquo; the  entry &ldquo;LCP Remote Entry&rdquo; is describing the VMs running on other host transport nodes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">get logical-switch c9390ed0-c587-4214-b26e-26705d4cd2f0 mac-table
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">Thu Jan 02 2025 UTC 09:37:43.375
</span></span><span class="line"><span class="cl">                         Logical Switch MAC Table
</span></span><span class="line"><span class="cl">---------------------------------------------------------------------------
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                             Host Kernel Entry
</span></span><span class="line"><span class="cl">===========================================================================
</span></span><span class="line"><span class="cl">     Inner MAC            Outer MAC            Outer IP      Flags
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                             LCP Remote Entry
</span></span><span class="line"><span class="cl">===========================================================================
</span></span><span class="line"><span class="cl">     Inner MAC            Outer MAC            Outer IP
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                              LCP Local Entry
</span></span><span class="line"><span class="cl">===========================================================================
</span></span><span class="line"><span class="cl">     Inner MAC            Outer MAC            Outer IP
</span></span><span class="line"><span class="cl"> 00:50:56:8d:0c:dd    00:50:56:61:ec:3c        10.0.2.13
</span></span></code></pre></div><p>As comparison the following output shows the assignment of the TEP interface for segment &ldquo;seg-tanzu-infra&rdquo;</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">get logical-switch dcbd8196-5550-466e-9bae-2c4f08476386  mac-table
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">Thu Jan 02 2025 UTC 09:38:01.367
</span></span><span class="line"><span class="cl">                         Logical Switch MAC Table
</span></span><span class="line"><span class="cl">---------------------------------------------------------------------------
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                             Host Kernel Entry
</span></span><span class="line"><span class="cl">===========================================================================
</span></span><span class="line"><span class="cl">     Inner MAC            Outer MAC            Outer IP      Flags
</span></span><span class="line"><span class="cl"> 00:50:56:8d:fe:35    00:50:56:6e:30:1e        10.0.2.11       0xf
</span></span><span class="line"><span class="cl"> 00:50:56:8d:6b:6c    00:50:56:6d:f3:87        10.0.2.14       0xf
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                             LCP Remote Entry
</span></span><span class="line"><span class="cl">===========================================================================
</span></span><span class="line"><span class="cl">     Inner MAC            Outer MAC            Outer IP
</span></span><span class="line"><span class="cl"> 00:50:56:8d:fe:35    00:50:56:6e:30:1e        10.0.2.11
</span></span><span class="line"><span class="cl"> 00:50:56:8d:6b:6c    00:50:56:6d:f3:87        10.0.2.14
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                              LCP Local Entry
</span></span><span class="line"><span class="cl">===========================================================================
</span></span><span class="line"><span class="cl">     Inner MAC            Outer MAC            Outer IP
</span></span><span class="line"><span class="cl"> 00:50:56:8d:27:f0    00:50:56:6b:71:aa        10.0.2.12
</span></span></code></pre></div><p>Output of the TEP interfaces including IP of the host transport node (vmk10 and vmk11).</p>
<pre tabindex="0"><code class="language-Palintext" data-lang="Palintext">esxcli network ip interface ipv4 get
</code></pre><pre tabindex="0"><code class="language-Palintext" data-lang="Palintext">Name   IPv4 Address  IPv4 Netmask   IPv4 Broadcast   Address Type  Gateway   DHCP DNS
-----  ------------  -------------  ---------------  ------------  --------  --------
vmk0   10.0.1.101    255.255.255.0  10.0.1.255       STATIC        10.0.1.1     false
vmk10  10.0.2.12     255.255.255.0  10.0.2.255       STATIC        10.0.2.1     false
vmk11  10.0.2.13     255.255.255.0  10.0.2.255       STATIC        10.0.2.1     false
vmk50  169.254.1.1   255.255.0.0    169.254.255.255  STATIC        0.0.0.0      false
</code></pre><p>Output of edge node from before and after the changes is visible in the section <a href="https://sdn-techtalk.com/posts/multitep-ha/#what-is-multi-tep-and-tep-group-ha">&ldquo;What is Multi-TEP and TEP Group HA?&rdquo;</a>.</p>
<h3 id="test-case-execution-before-applying-the-changes">Test case execution before applying the changes</h3>
<p>Before validating the behavoiour after the changes, lets start with the behavior before all the optimisations.</p>
<h4 id="multi-tep-ha---host-transport-nodes">Multi-TEP HA - Host Transport Nodes</h4>
<p>After the layer2 communication is interrupted, the VM is still assigned to the affected TEP IP, as shown below.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">get logical-switch c9390ed0-c587-4214-b26e-26705d4cd2f0 mac-table
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">Thu Jan 02 2025 UTC 10:53:31.084
</span></span><span class="line"><span class="cl">                         Logical Switch MAC Table
</span></span><span class="line"><span class="cl">---------------------------------------------------------------------------
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                             Host Kernel Entry
</span></span><span class="line"><span class="cl">===========================================================================
</span></span><span class="line"><span class="cl">     Inner MAC            Outer MAC            Outer IP      Flags
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                             LCP Remote Entry
</span></span><span class="line"><span class="cl">===========================================================================
</span></span><span class="line"><span class="cl">     Inner MAC            Outer MAC            Outer IP
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                              LCP Local Entry
</span></span><span class="line"><span class="cl">===========================================================================
</span></span><span class="line"><span class="cl">     Inner MAC            Outer MAC            Outer IP
</span></span><span class="line"><span class="cl"> 00:50:56:8d:0c:dd    00:50:56:61:ec:3c        10.0.2.13
</span></span></code></pre></div><p>The ICMP output proves that the VM is no longer reachable as expected.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=33 ttl=60 time=3.97 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=34 ttl=60 time=2.50 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=35 ttl=60 time=5.38 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=36 ttl=60 time=8.48 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=37 ttl=60 time=8.03 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=38 ttl=60 time=14.3 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=39 ttl=60 time=2.78 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=40 ttl=60 time=24.9 ms
</span></span><span class="line"><span class="cl">--- 10.100.12.2 ping statistics ---
</span></span><span class="line"><span class="cl">55 packets transmitted, 40 received, 27% packet loss, time 54399ms
</span></span><span class="line"><span class="cl">rtt min/avg/max/mdev = 2.509/10.785/49.817/9.683 ms
</span></span></code></pre></div><p>But as already mentioned NSX is able to failover without the implementation of the Multi-TEP HA feature, if the link state is changing from &ldquo;UP&rdquo; to &ldquo;DOWN&rdquo;.
In this case the TEP IP will be moved from the failed vmnic to the any other vmnic, wich is still working. An example is shown below.</p>
<p>Mapping between vmk11 and vmnics before failover. The output is discoverd by the command &ldquo;esxtop&rdquo; in the networking overview.</p>
<p><a href="esxtop-before-failover.png"><img src="esxtop-before-failover.png" alt="esxtop-before-failover"></a></p>
<p>Mapping between vmk11 and vmnics after failover.</p>
<p><a href="esxtop-after-failover.png"><img src="esxtop-after-failover.png" alt="esxtop-after-failover"></a></p>
<p>The ICMP protocols shows a very fast failover after only one packet is lost.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=6 ttl=60 time=6.02 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=7 ttl=60 time=23.2 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=8 ttl=60 time=31.4 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=9 ttl=60 time=8.55 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=10 ttl=60 time=7.19 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=11 ttl=60 time=19.0 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=12 ttl=60 time=51.3 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=13 ttl=60 time=79.2 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=15 ttl=60 time=13.6 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=16 ttl=60 time=17.0 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=17 ttl=60 time=10.0 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=18 ttl=60 time=11.0 ms
</span></span><span class="line"><span class="cl">^C
</span></span><span class="line"><span class="cl">--- 10.100.12.2 ping statistics ---
</span></span><span class="line"><span class="cl">18 packets transmitted, 17 received, 5% packet loss, time 17043ms
</span></span><span class="line"><span class="cl">rtt min/avg/max/mdev = 5.584/23.531/79.297/20.795 ms
</span></span></code></pre></div><h4 id="tep-groups---edge-transport-nodes">TEP Groups - Edge Transport Nodes</h4>
<p>After breaking the layer2 connectivity of fp-eth1 where the segment of the alpine VM is assigned to, the alpine loses the reachability. This is also shown in the following ICMP protocol.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=31 ttl=60 time=32.4 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=32 ttl=60 time=18.4 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=33 ttl=60 time=11.5 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=34 ttl=60 time=6.23 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=35 ttl=60 time=4.79 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=36 ttl=60 time=17.9 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=37 ttl=60 time=3.63 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=38 ttl=60 time=7.20 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=39 ttl=60 time=8.14 ms
</span></span><span class="line"><span class="cl">^C
</span></span><span class="line"><span class="cl">--- 10.100.12.2 ping statistics ---
</span></span><span class="line"><span class="cl">46 packets transmitted, 38 received, 17% packet loss, time 45225ms
</span></span><span class="line"><span class="cl">rtt min/avg/max/mdev = 2.273/9.839/32.415/6.569 ms
</span></span></code></pre></div><p>A test for a link failure is not realistic, since it is a VM, but it would work exactly like the ESXi host and the TEP IP would move to the fp-eth interface which is still available.
But you should keep in mind that VLAN backed segments on the edge nodes will not failover, if the link is down. Therfore it is higly reommended to use named teaming policies where redundancy is required, like for the T0 uplinks.
In the case of T0 router tha failovermight be handled over BGP, in case at least one of the BGP uplinks is up and running.</p>
<h3 id="test-case-execution-after-the-changes-are-applied">Test case execution after the changes are applied</h3>
<p>Now that we are familiar with the behavior before the changes, let&rsquo;s check what has been improved</p>
<h4 id="multi-tep-ha---host-transport-nodes-1">Multi-TEP HA - Host Transport Nodes</h4>
<p>After breaking the layer2 communication of the ESXi host where the VM was running, the assigned TEP IP was changed from 10.0.2.13 to 10.0.2.12, as shown below.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">get logical-switch c9390ed0-c587-4214-b26e-26705d4cd2f0 mac-table
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">Thu Jan 02 2025 UTC 10:24:49.329
</span></span><span class="line"><span class="cl">                         Logical Switch MAC Table
</span></span><span class="line"><span class="cl">---------------------------------------------------------------------------
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                             Host Kernel Entry
</span></span><span class="line"><span class="cl">===========================================================================
</span></span><span class="line"><span class="cl">     Inner MAC            Outer MAC            Outer IP      Flags
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                             LCP Remote Entry
</span></span><span class="line"><span class="cl">===========================================================================
</span></span><span class="line"><span class="cl">     Inner MAC            Outer MAC            Outer IP
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                              LCP Local Entry
</span></span><span class="line"><span class="cl">===========================================================================
</span></span><span class="line"><span class="cl">     Inner MAC            Outer MAC            Outer IP
</span></span><span class="line"><span class="cl"> 00:50:56:8d:0c:dd    00:50:56:6b:71:aa        10.0.2.12
</span></span></code></pre></div><p>As prove of the reachability and failover time you can check the following ICMP output. Here you can see that 10 packets were lost witin the time of the failover.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=33 ttl=60 time=110 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=34 ttl=60 time=6.79 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=35 ttl=60 time=3.06 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=45 ttl=60 time=8.99 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=46 ttl=60 time=6.65 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=47 ttl=60 time=11.8 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=48 ttl=60 time=6.23 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=49 ttl=60 time=9.94 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=50 ttl=60 time=6.40 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=51 ttl=60 time=8.25 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=52 ttl=60 time=5.65 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=53 ttl=60 time=5.71 ms
</span></span><span class="line"><span class="cl">^C
</span></span><span class="line"><span class="cl">--- 10.100.12.2 ping statistics ---
</span></span><span class="line"><span class="cl">53 packets transmitted, 43 received, 18% packet loss, time 52262ms
</span></span><span class="line"><span class="cl">rtt min/avg/max/mdev = 2.282/16.680/110.415/25.816 ms
</span></span></code></pre></div><p>Further you can check the GENEVE tunnels from the NSX Manager UI. Based on this output all GENEVE tunnels from TEP IP 10.0.2.13 are gone and you see the tunnels from 10.0.2.12 only.</p>
<p><a href="tunneloverview-tepha-failover.png"><img src="tunneloverview-tepha-failover.png" alt="tunneloverview-tepha-failover"></a></p>
<h4 id="tep-groups---edge-transport-nodes-1">TEP Groups - Edge Transport Nodes</h4>
<p>After breaking the layer2 connection of one TEP interface, the connection was shortly interrupted and recovered after few seconds.
As shown within the ICMP protocol below, just three packets were lost.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=1 ttl=60 time=6.25 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=2 ttl=60 time=3.54 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=3 ttl=60 time=28.0 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=4 ttl=60 time=3.14 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=5 ttl=60 time=4.47 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=6 ttl=60 time=4.53 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=7 ttl=60 time=40.0 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=11 ttl=60 time=6.80 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=12 ttl=60 time=13.3 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=13 ttl=60 time=4.11 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=14 ttl=60 time=4.38 ms
</span></span><span class="line"><span class="cl">64 bytes from 10.100.12.2: icmp_seq=15 ttl=60 time=35.4 ms
</span></span><span class="line"><span class="cl">^C
</span></span><span class="line"><span class="cl">--- 10.100.12.2 ping statistics ---
</span></span><span class="line"><span class="cl">15 packets transmitted, 12 received, 20% packet loss, time 14073ms
</span></span><span class="line"><span class="cl">rtt min/avg/max/mdev = 3.145/12.842/40.042/12.995 ms
</span></span></code></pre></div><p>Further you can inspect the GENEVE tunnels of the edge node, where you can see some tunnels in state &ldquo;DOWN&rdquo; based on &ldquo;1 - Control Detection Time Expired&rdquo;.</p>
<p><a href="edgetunneldown.png"><img src="edgetunneldown.png" alt="edgetunneldown"></a></p>
<h2 id="summary">Summary</h2>
<p>After testing the new features of Multi-TEP and TEP Group high availability I can say this is really a great improvemnent of the failover behavior.
From my point of view this is a very important step in case of resilience and reducing the impact of many failure scenarios like software bugs in the underlay network wich might cause the physical switches to freeze.
Further it also improves the load sharing, since it is flow based after TEP groups are enabled compared to have load sharing just available segment wise. This could also increase the total bandwitdh possible for a single VM, which is now load-shared per flow instead stically mapped to one fastpath interface of an edge transport node.</p>
<p>Since this feature is very easy to implement and will improve the failover behavior by optiomizing the BFD sessions and load sharing mechanism on the edge nodes, I would highly recommend to implement those two features.
Sure the features are quite new and there might be some bugs, but I did not find any unexpected behavior so far. Anyways it would always be the best to test new features in dev before implementation in production.</p>
<h2 id="further-resources">Further resources</h2>
<p>If you are interested in a more detailed perfomance tests my check out the following blog post of my esteemed colleague Daniel Krieger. He is comparing the performance with the TEP group feature disabled and enabled and how it influences the overall North-South throughput under real world conditions.</p>
<ul>
<li><a href="https://sdn-warrior.org/posts/nsx-tep-groups/">More performance trough NSX Edge TEP groups?</a></li>
</ul>
<p>Further it is always worth to check the official doctumentation for Multi-TEP HA.</p>
<ul>
<li><a href="https://techdocs.broadcom.com/us/en/vmware-cis/nsx/vmware-nsx/4-2/administration-guide/host-switches/multi-tep-high-availability.html">VMware NSX Administration Guide: Multi-TEP High Availability</a></li>
</ul>
]]></content>
		</item>
		
		<item>
			<title>Antrea Egress - Controlling the egress IPs for K8s Pods within Tanzu</title>
			<link>https://sdn-techtalk.com/posts/antrea-egress/</link>
			<pubDate>Sat, 21 Dec 2024 19:54:18 +0100</pubDate>
			
			<guid>https://sdn-techtalk.com/posts/antrea-egress/</guid>
			<description><![CDATA[%!s(<nil>)]]></description>
			<content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>The integration of K8s platforms within a enterprise network environment is always a challenge regarding security and routing of certain K8s pods. Multiple pods are running in the same K8s cluster but might have different functinalities and requirements to access services outside of the K8s platform.
What should you do, if you are unable to assign a specific outgoing IP to a specific pod or group of pods? In this case you are forced to treat all the pods within a specific K8s cluster the same way and create firewall rules based on this.
That does mean all the different pods within a K8s cluster will have the same network permissions and it is very challenging to implement further isolation.</p>
<p>For legacy applications which were deployed on physical servers or specific virtual servers, each of those servers got a specific IP assigned. Based on this it was possible to create specific firewall rules for the specific servers based on the different requirements to access other services or beeing accessed from other systems.
This brought some isolation and overall security to the enterprise networks and is also a requirement to satisfy the guidelines of some specific authorities like BAFIN or based on DORA.</p>
<p>Why is this now a challenge within K8s? K8s is an additinal layer of abstraction to increase the efficiency to deploy different services on a single server. The layer of K8s will be installed ontop of an operationg system, which is either installed on a physical or virtual server.
Since the most enterprises does have a high ratio of virtualisation, it is more likely that K8s is deployed ontop of a VM or group of VMs to build a K8s cluster. Each of those VMs is now called K8s worker or master and will run one or multiple K8s services. In this case all the different VMs still have a specific IP adress that can be used in firewall rules, but the K8s services are getting deployed ontop of thoese VMs.
That means multiple K8s services are sharing the same group of VMs and if a K8s service wants to reach a different service outside of K8s it will leave the K8s worker or master by using the IP of those VMs.</p>
<p>Nevertheless there are multiple solutions for this challenge. I would like to tell you about one solution that can be used in the K8s platform by VMware Tanzu (VKS) and requires the K8s network plugin (CNI) &ldquo;Antrea&rdquo;.
The specific feature is called &ldquo;Antrea Engress&rdquo;.</p>
<p>I will not show how to deploy the Tanzu supervisor cluster, but how to create a K8s guest cluster with the required features enabled and how to assign specific egress IPs to specific K8s pods.</p>
<h2 id="lab-environment">Lab environment</h2>
<p>For the tests I worked with the following lab environment.</p>
<ul>
<li>Three nested ESXi hosts of version 8.0.3, 24022510</li>
<li>vCenter server of version 8.0.3, 24022515</li>
<li>NSX Datacenter version 4.2.1.1</li>
<li>Tanzu supervisor cluster integrated with NSX Datacenter</li>
</ul>
<h2 id="requirements">Requirements</h2>
<p>My tests are based on assigning seperate egress subnets to different K8s pods within the same K8s cluster. To implement this solution the following versions are required.</p>
<ul>
<li>TKr 1.31</li>
<li>TKG Services 3.2.0</li>
</ul>
<p>Further it is required to deploy Tanzu with NSX as networking layer and using Antrea as CNI.
For NSX a specific feature called &ldquo;Child Segments&rdquo; will be used to get this implementation done, so take care your current NSX version is supporting this feature.</p>
<p>The next requirement is optional, but does influence the overall configuration that needs to be done manually.
If you are using NAT mode, you need to manually create NoSNAT rules for the NSX child segments that will be cerated within the process. If NAT mode is disabled, this manuall configuration is not required.</p>
<h2 id="nsx-child-segments">NSX child segments</h2>
<p>Before digging into the steps how to assign seperate egress subnets to different K8s pods, I will shortly describe what NSX child segments are.
If you are not aware of NSX and the routers within NSX, you need to familiarize with that first. Therefore I would recommend the NSX reference design guide <a href="https://community.broadcom.com/viewdocument/nsx-reference-design-guide-42-v10?CommunityKey=b76535ef-c5a2-474d-8270-3e83685f020e&amp;tab=librarydocuments">https://community.broadcom.com/viewdocument/nsx-reference-design-guide-42-v10?CommunityKey=b76535ef-c5a2-474d-8270-3e83685f020e&amp;tab=librarydocuments</a>.</p>
<p>NSX child segents are based on a parent segment which is connected to a T1 router. The child segment it self can be either connected to the same or a different T1 router than the parent segment.
The connection between the parent and the child segment will be done by a VLAN tag and two child segments cannot share the same VLAN tag.</p>
<p>This feature is currently not available in the UI and those child segments can be wither created through the NSX API or the Antrea NSX control application.</p>
<p>Based on this feature different pods can be assigned to different subnets without the requirement of assigning different portgroups to the K8s worker and master nodes. Those subnets also does not interfere withe the reserved IPs for the assigned namespace CIDR of the K8s clusters.</p>
<p>More information of the child sgements can be discoverd in the documentation of Broadcom. <a href="https://techdocs.broadcom.com/us/en/vmware-cis/nsx/vmware-nsx/4-2/administration-guide/segments/creating-a-child-segment.html">https://techdocs.broadcom.com/us/en/vmware-cis/nsx/vmware-nsx/4-2/administration-guide/segments/creating-a-child-segment.html</a></p>
<h2 id="prepering-the-k8s-guest-cluster">Prepering the K8s guest cluster</h2>
<p>Before creating the K8s cluster, it is required to create the Antrea configuration to enable the feature &ldquo;EgressSeperateSubnet&rdquo;.
The following example shows an example configuration. This configuration does not negate the default settings of Antrea, which are not mentioned in this YAML-File.
I also added the NSX integration in my configuration to integrate the NSX Distributed Firewall with Antrea, but this will not be further discussed in this article. Further the feature of &ldquo;NodePortLocal&rdquo; is also not required for the assignment of dedicated egress subnets.</p>
<p>The name of the K8s object &ldquo;AntreaConfig&rdquo; is very important and should match the prefix &lt;K8s cluster name&gt;-antrea-package, otherwhise the Antrea config will not be assigned. Further this configuration needs to be added before the K8s cluster is created.
It is also possible to adjust the configuration after the deployment of the K8s cluster, but this is much more complex and required a reboot of the K8s nodes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">cni.tanzu.vmware.com/v1alpha1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">AntreaConfig</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">tkc-test01-antrea-package</span><span class="w"> </span><span class="c">#prefix required</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"> </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">test</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">antrea</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">config</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">featureGates</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">EgressSeparateSubnet</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">NodePortLocal</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">antreaNSX</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">enable</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w"> </span><span class="c">#false by default</span><span class="w">
</span></span></span></code></pre></div><p>After the configuration is applied, you can start with the K8s cluster deployment. Following my example K8s cluster YAML-File.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">run.tanzu.vmware.com/v1alpha3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">TanzuKubernetesCluster</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">tkc-test01</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">test</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">topology</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">controlPlane</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">vmClass</span><span class="p">:</span><span class="w"> </span><span class="l">best-effort-medium</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">storageClass</span><span class="p">:</span><span class="w"> </span><span class="l">labnfs</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">tkr</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">reference</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="c">#name: v1.29.4---vmware.3-fips.1-tkg.1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">v1.31.1---vmware.2-fips-vkr.2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">nodePools</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">worker</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">vmClass</span><span class="p">:</span><span class="w"> </span><span class="l">best-effort-medium</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">storageClass</span><span class="p">:</span><span class="w"> </span><span class="l">labnfs</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">settings</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">network</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">pods</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">cidrBlocks</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;192.168.0.0/16&#34;</span><span class="p">]</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">services</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">cidrBlocks</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;10.196.0.0/12&#34;</span><span class="p">]</span><span class="w">
</span></span></span></code></pre></div><h2 id="creating-test-k8s-deployments">Creating test K8s deployments</h2>
<p>As a next step I created two namespaces to assign two different egress subnets later on.</p>
<ul>
<li>Namespace 1: prod</li>
<li>Namespace 2: staging</li>
</ul>
<p>In each of those namespaces I created a test deployment based on the follwoing YAML-File.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">web</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">web</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">web</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx:latest</span><span class="w">
</span></span></span></code></pre></div><p>The commands used for this deployment are shown below.</p>
<p>Creating namespaces:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">kubectl create ns &lt;namespace name&gt;
</span></span></code></pre></div><p>Creating deployment:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">kubectl apply -f web.yaml -n &lt;namespace name&gt;
</span></span></code></pre></div><p>In addition to the K8s deployments I deployed a docker container outside my NSX environment based on the following steps.
The docker caontainer will run a small web application, which responds with the IP used to access this application. Based on this it will be easy to validate which IP will be used by the different K8s Pods and if the egress IP are assigned as expected.</p>
<ol>
<li>Create folder your Dockerfile and create the Dockerfile</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Dockerfile" data-lang="Dockerfile"><span class="line"><span class="cl"><span class="k">FROM</span><span class="s"> tiangolo/uvicorn-gunicorn-fastapi:python3.9</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">COPY</span> ./app /app<span class="err">
</span></span></span></code></pre></div><ol start="2">
<li>Create a subolder called &ldquo;app&rdquo;</li>
<li>Create a file with the name &ldquo;main.py&rdquo; in the subfolder &ldquo;app&rdquo; with the content below.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Py" data-lang="Py"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">fastapi</span> <span class="kn">import</span> <span class="n">FastAPI</span><span class="p">,</span> <span class="n">Request</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">fastapi.responses</span> <span class="kn">import</span> <span class="n">HTMLResponse</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">app</span> <span class="o">=</span> <span class="n">FastAPI</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@app.get</span><span class="p">(</span><span class="s2">&#34;/&#34;</span><span class="p">,</span> <span class="n">response_class</span><span class="o">=</span><span class="n">HTMLResponse</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">async</span> <span class="k">def</span> <span class="nf">index</span><span class="p">(</span><span class="n">request</span><span class="p">:</span> <span class="n">Request</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">client_host</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">host</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="s2">&#34;Requester IP: &#34;</span> <span class="o">+</span> <span class="n">client_host</span> <span class="o">+</span> <span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span></code></pre></div><ol start="4">
<li>Go back to the folder where the Dockerfile is located and build the docker image</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">docker build -t showfastip .
</span></span></code></pre></div><ol start="5">
<li>Run the docker imaged with a choosen port, which is not in use. I decided to use external port 88</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">docker run -d -p 88:80 showfastip
</span></span></code></pre></div><h2 id="create-child-segments">Create child segments</h2>
<p>I decided to create the child segments with the commandline tool &ldquo;antreansxctl&rdquo;. This commandline tool can be either downloaded to your local PC or you can connect to the Antrea pod of your K8s cluster to execute the desired command.</p>

    <aside class="admonition warning">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-alert-circle">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="8" x2="12" y2="12"></line>
      <line x1="12" y1="16" x2="12.01" y2="16"></line>
   </svg></div><b>It&#39;s important to note that the antreansxctl commandline tool is provided in the internetworking pod</b>
        </div>
        <div class="admonition-content">This Pod will be deployed, if you enable the feature antreaNSX show above in the AntreaConfig example.</div>
    </aside>
<p>A connection to the internetworking pod can be done by the kubectl command below.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">kubectl exec -n vmware-system-antrea interworking-5bdd8f5b5f-wd6wg -c mp-adapter -it -- bash
</span></span></code></pre></div><p>The command to create a child segment requires the follwoing information.</p>
<ul>
<li>Path of the parent segment: can be gathered via NSX API by the code &ldquo;curl &ndash;location &ndash;request GET &lsquo;<a href="https://nsx.lab.home/policy/api/v1/infra/segments/'">https://nsx.lab.home/policy/api/v1/infra/segments/'</a> &ndash;header &lsquo;Content-Type: application/json&rsquo; &ndash;header &lsquo;Authorization: Basic <!-- raw HTML omitted -->&rsquo;&rdquo;</li>
<li>NSX Manager IP</li>
<li>NSX Manager User and Password</li>
<li>VLAN ID used to tag the child segment (this VLAN ID will not be visible outside of NSX and will to overlap with any VLAN on the underlay network)</li>
<li>CIDR and Gateway IP of the child segmnet (the gateway IP will be created on the T1 router used for the child segment, which is the same T1 as for the parent T1 in this case)</li>
</ul>
<p>Command description:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">antreansxctl child-segment-create --nsx-managers=&lt;NSX Manager IP&gt; --user=admin --password=&#39;&lt;NSX Manager admin password&gt;&#39; --cidr=&#34;&lt;CIDR child segment&gt;&#34; --gateway=&#34;&lt;Gateway IP of child segment&gt;&#34; --parent=&#34;&lt;path of parent segment&gt;&#34; --vlan=&lt;vlan id&gt; &lt;name of child segment&gt;
</span></span></code></pre></div><p>An example of this command is shown below.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">antreansxctl child-segment-create --nsx-managers=10.0.1.12 --user=admin --password=&#39;VMware1!VMware1!&#39; --cidr=&#34;10.100.5.0/25&#34; --gateway=&#34;10.100.5.1&#34; --parent=&#34;/infra/segments/vnet_60f55536-121d-40d0-ac48-4cb976decf3d_0&#34; --vlan=110 egress-prod
</span></span></code></pre></div><p>I executed the same command for staging with some other parameters.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">antreansxctl child-segment-create --nsx-managers=10.0.1.12 --user=admin --password=&#39;VMware1!VMware1!&#39; --cidr=&#34;10.100.5.128/30&#34; --gateway=&#34;10.100.5.129&#34; --parent=&#34;/infra/segments/vnet_60f55536-121d-40d0-ac48-4cb976decf3d_0&#34; --vlan=120 egress-staging
</span></span></code></pre></div><p>The two child segments are now visible in the NSX UI as shown in the following picture.
<a href="childsegmentsui.png"><img src="childsegmentsui.png" alt="Childsegents"></a></p>
<h2 id="create-and-assign-antrea-external-ip-pools">Create and assign antrea external ip pools</h2>
<p>I created one external IP pool for staging and one for prod.</p>
<p>Prod external IP Pool YAML-File:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">crd.antrea.io/v1beta1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ExternalIPPool</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prod-external-ip-pool</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ipRanges</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">start</span><span class="p">:</span><span class="w"> </span><span class="m">10.100.5.10</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">end</span><span class="p">:</span><span class="w"> </span><span class="m">10.100.5.20</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">subnetInfo</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">gateway</span><span class="p">:</span><span class="w"> </span><span class="m">10.100.5.1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">prefixLength</span><span class="p">:</span><span class="w"> </span><span class="m">25</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">vlan</span><span class="p">:</span><span class="w"> </span><span class="m">110</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">nodeSelector</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span></span></span></code></pre></div><p>Staging external IP pool YAML-File:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">crd.antrea.io/v1beta1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ExternalIPPool</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">staging-external-ip-pool</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ipRanges</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">start</span><span class="p">:</span><span class="w"> </span><span class="m">10.100.5.130</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">end</span><span class="p">:</span><span class="w"> </span><span class="m">10.100.5.130</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">subnetInfo</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">gateway</span><span class="p">:</span><span class="w"> </span><span class="m">10.100.5.129</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">prefixLength</span><span class="p">:</span><span class="w"> </span><span class="m">30</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">vlan</span><span class="p">:</span><span class="w"> </span><span class="m">120</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">nodeSelector</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span></span></span></code></pre></div><p>Creating the two IP pools:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">kubectl apply -f antrea-externalippool-prod.yaml
</span></span><span class="line"><span class="cl">kubectl apply -f antrea-externalippool-staging.yaml
</span></span></code></pre></div><p>YAML-Files for assignment of prod IP pool to prod web deployment:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">crd.antrea.io/v1beta1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Egress</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">egress-prod-web</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">appliedTo</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">namespaceSelector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">kubernetes.io/metadata.name</span><span class="p">:</span><span class="w"> </span><span class="l">prod</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">podSelector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">web</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">externalIPPool</span><span class="p">:</span><span class="w"> </span><span class="l">prod-external-ip-pool</span><span class="w">
</span></span></span></code></pre></div><p>YAML-Files for assignment of staging IP pool to prod web deployment:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">crd.antrea.io/v1beta1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Egress</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">egress-staging-web</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">appliedTo</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">namespaceSelector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">kubernetes.io/metadata.name</span><span class="p">:</span><span class="w"> </span><span class="l">staging</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">podSelector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">web</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">externalIPPool</span><span class="p">:</span><span class="w"> </span><span class="l">staging-external-ip-pool</span><span class="w">
</span></span></span></code></pre></div><p>Assigning  the external IP pools:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">kubectl apply -f egress-prod-web.yaml -n prod
</span></span><span class="line"><span class="cl">kubectl apply -f egress-staging-web.yaml -n staging
</span></span></code></pre></div><p>The following command shows the assigned IPs of the two IP pools.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">kubectl get egress
</span></span><span class="line"><span class="cl">NAME                 EGRESSIP       AGE   NODE
</span></span><span class="line"><span class="cl">egress-prod-web      10.100.5.10    42s   tkc-test01-hsqsg-vmm2w
</span></span><span class="line"><span class="cl">egress-staging-web   10.100.5.130   28s   tkc-test01-worker-s9s27-qqhrg-z6jd5
</span></span></code></pre></div><h2 id="create-nosnat-rule">Create noSNAT rule</h2>
<p>As already mentioned it is required to manually configure a noSNAT rule for the external IP pool subnets, if NAT mode is enabled for the used vSphere Namespace where the K8s cluster is deployed.
The noSNAT rule needs to be configured on the T1 router where the child segments are connected as shown in the picture below.</p>
<p><a href="nosnat.png"><img src="nosnat.png" alt="noSNAT"></a></p>
<h2 id="testing-if-egress-ip-pool-assigment-is-working">Testing if egress ip pool assigment is working</h2>
<p>The following tests are showing an access from the web pod in both namespaces and which external IP will be used to access the webserver deployed in dockers as mentioned earlier.
The Requester IP is the egress IP visible for the target web serverrunning in docker.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Plaintext" data-lang="Plaintext"><span class="line"><span class="cl">stevenschramm@Stevens-MacBook-Pro homelab % kubectl -n prod exec web-54d6b8f598-7ddqv -- curl -s 192.168.178.222:88
</span></span><span class="line"><span class="cl">Requester IP: 10.100.5.10
</span></span><span class="line"><span class="cl">stevenschramm@Stevens-MacBook-Pro homelab % kubectl -n staging exec web-54d6b8f598-slnm8 -- curl -s 192.168.178.222:88
</span></span><span class="line"><span class="cl">Requester IP: 10.100.5.130
</span></span></code></pre></div>]]></content>
		</item>
		
	</channel>
</rss>
